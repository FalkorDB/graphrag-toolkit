{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "434fea4e",
   "metadata": {},
   "source": [
    "# 02 - Separate Extract and Build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb5cff",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven't already, install the toolkit and dependencies using the [Setup](./00-Setup.ipynb) notebook."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup local development",
   "id": "aa156eede34e0d07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check GPU support on Ubuntu 24.04 LTS",
   "id": "e2931c39a821a295"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:07.116128Z",
     "start_time": "2025-04-26T21:52:07.114020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Force inject paths (adapt to your system if different)\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "os.environ[\"PATH\"] += \":/usr/local/cuda/bin\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Re-check CUDA\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Version (Torch):\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"GPU still not detected inside notebook\")"
   ],
   "id": "4411aec46b8c3a41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3060\n",
      "CUDA Version (Torch): 12.6\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup AWS Profile",
   "id": "f282b69f9d609348"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:07.219364Z",
     "start_time": "2025-04-26T21:52:07.218112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure AWS Profile and Region\n",
    "from graphrag_toolkit.lexical_graph import GraphRAGConfig\n",
    "\n",
    "# Assign profile and region to GraphRAGConfig\n",
    "GraphRAGConfig.aws_profile = \"padmin\" #Optional, use if using AWS SSO\n",
    "GraphRAGConfig.aws_region = \"us-east-1\""
   ],
   "id": "47abae423e9ef07f",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup model",
   "id": "6d9a0e2edb78152b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:07.270943Z",
     "start_time": "2025-04-26T21:52:07.269822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set Claude model via Bedrock\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\""
   ],
   "id": "87c357cd092d81e0",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup BedrockConverse",
   "id": "442bd8d6316decc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:07.342640Z",
     "start_time": "2025-04-26T21:52:07.314954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure BedrockConverse\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "\n",
    "try:\n",
    "    GraphRAGConfig.extraction_llm = BedrockConverse.from_json(f'''\n",
    "    {{\n",
    "        \"model\": \"{model_id}\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"profile_name\": \"{GraphRAGConfig.aws_profile}\",\n",
    "        \"region_name\": \"{GraphRAGConfig.aws_region}\"\n",
    "    }}\n",
    "    ''')\n",
    "    print(f\"Successfully configured Bedrock model: {model_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize BedrockConverse: {str(e)}\")\n",
    "    raise\n",
    "### Display LLM Configuration"
   ],
   "id": "88f68bbcaed4dfec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully configured Bedrock model: us.anthropic.claude-3-5-sonnet-20240620-v1:0\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Display LLM Configuration",
   "id": "cb04f505f69ba89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:07.363675Z",
     "start_time": "2025-04-26T21:52:07.362062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display LLM configuration\n",
    "llm = GraphRAGConfig.extraction_llm\n",
    "print(\"LLM class:\", llm.__class__.__name__)\n",
    "print(\"Model ID:\", llm.model)\n",
    "print(\"Temperature:\", llm.temperature)\n",
    "print(\"Max tokens:\", llm.max_tokens)\n",
    "print(\"Profile:\", getattr(llm, 'profile_name', None))\n",
    "print(\"Region:\", getattr(llm, 'region_name', None))"
   ],
   "id": "4630589710022b6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM class: BedrockConverse\n",
      "Model ID: us.anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "Temperature: 0.0\n",
      "Max tokens: 4096\n",
      "Profile: padmin\n",
      "Region: us-east-1\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup GraphRag Config",
   "id": "c53811af8b0df4bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:07.410355Z",
     "start_time": "2025-04-26T21:52:07.408661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup GraphRag Config\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "import os\n",
    "from graphrag_toolkit.lexical_graph.config import GraphRAGConfig\n",
    "\n",
    "def setup_graphrag_config() -> None:\n",
    "    \"\"\"\n",
    "    Inject BedrockEmbedding into existing GraphRAGConfig without resetting other config values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use existing values if present, otherwise fallback to env vars\n",
    "        region = GraphRAGConfig.aws_region or os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "        profile = GraphRAGConfig.aws_profile or os.getenv(\"AWS_PROFILE\")\n",
    "\n",
    "        embed_model = BedrockEmbedding(\n",
    "            model=\"amazon.titan-embed-text-v1\",\n",
    "            region=region,\n",
    "            profile_name=profile\n",
    "        )\n",
    "\n",
    "        GraphRAGConfig.embed_model = embed_model\n",
    "        GraphRAGConfig.embed_dimensions = 1536\n",
    "\n",
    "        print(\"GraphRAGConfig updated with Bedrock embedding model (non-destructive).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to update GraphRAGConfig: {str(e)}\")\n",
    "        raise"
   ],
   "id": "159b30f26ef39b9b",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Apply config and verify embedding generation",
   "id": "9e075fdd39434ea7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:08.288161Z",
     "start_time": "2025-04-26T21:52:07.453208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply config and verify embedding generation\n",
    "setup_graphrag_config()\n",
    "\n",
    "# Test embedding generation using the LlamaIndex Bedrock embedder\n",
    "texts = [\"This is a sample text.\", \"Another text for embedding.\"]\n",
    "\n",
    "try:\n",
    "    print(f\"Sending embedding request for texts: {texts}\")\n",
    "\n",
    "    # Get embeddings directly from the updated global config\n",
    "    embeddings = GraphRAGConfig.embed_model.get_text_embedding_batch(texts)\n",
    "\n",
    "    # Show a preview\n",
    "    print(f\"Generated embeddings (first two dims):\")\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        print(f\"Text {i}: {emb[:2]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during embedding generation: {str(e)}\")\n",
    "    raise"
   ],
   "id": "2fdc035eb573462",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphRAGConfig updated with Bedrock embedding model (non-destructive).\n",
      "Sending embedding request for texts: ['This is a sample text.', 'Another text for embedding.']\n",
      "Generated embeddings (first two dims):\n",
      "Text 0: [-0.640625, -0.035400390625]\n",
      "Text 1: [-0.3828125, 0.0830078125]\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup connection with PostgreSQL",
   "id": "9ea53fd883c9760f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:08.296161Z",
     "start_time": "2025-04-26T21:52:08.293562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to PostgreSQL Vector Store\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "\n",
    "# PostgreSQL connection string\n",
    "postgre_connection_info = 'postgresql://graphrag:graphragpass@localhost:5432/graphrag_db'\n",
    "\n",
    "# Instantiate vector store using factory\n",
    "vector_store = VectorStoreFactory.for_vector_store(postgre_connection_info)\n",
    "\n",
    "# Optional: confirm\n",
    "print(f\"Vector store initialized: {vector_store}\")"
   ],
   "id": "1d6a660442963b7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized: indexes={'chunk': PGIndex(index_name='chunk', tenant_id=TenantId(value=None), writeable=True, database='graphrag_db', schema_name='graphrag', host='localhost', port=5432, username='graphrag', password='graphragpass', dimensions=1536, embed_model=BedrockEmbedding(model_name='amazon.titan-embed-text-v1', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7b285c34c4a0>, num_workers=None, profile_name='padmin', aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None, region_name=None, botocore_session=None, botocore_config=None, max_retries=10, timeout=60.0, additional_kwargs={}), enable_iam_db_auth=False, initialized=False), 'statement': PGIndex(index_name='statement', tenant_id=TenantId(value=None), writeable=True, database='graphrag_db', schema_name='graphrag', host='localhost', port=5432, username='graphrag', password='graphragpass', dimensions=1536, embed_model=BedrockEmbedding(model_name='amazon.titan-embed-text-v1', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7b285c34c4a0>, num_workers=None, profile_name='padmin', aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None, region_name=None, botocore_session=None, botocore_config=None, max_retries=10, timeout=60.0, additional_kwargs={}), enable_iam_db_auth=False, initialized=False)}\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup connection with FalkorDB",
   "id": "e1ae1da0b4b6e51d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:10.241689Z",
     "start_time": "2025-04-26T21:52:08.339011Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.4.0.zip#subdirectory=lexical-graph-contrib/falkordb",
   "id": "196595c2fc1e0405",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.4.0.zip#subdirectory=lexical-graph-contrib/falkordb\r\n",
      "  Using cached https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.4.0.zip\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: FalkorDB in /home/evanerwee/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages (from graphrag-toolkit-lexical-graph-falkordb==1.0.0) (1.1.1)\r\n",
      "Requirement already satisfied: redis in /home/evanerwee/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages (from graphrag-toolkit-lexical-graph-falkordb==1.0.0) (5.2.1)\r\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:10.248682Z",
     "start_time": "2025-04-26T21:52:10.247048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to FalkorDB Graph Store\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "\n",
    "# Connection string for FalkorDB\n",
    "falkordb_connection_info = 'falkordb://localhost:6379'\n",
    "\n",
    "# Register the FalkorDB backend with the factory\n",
    "GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "# Instantiate a graph store using the factory\n",
    "graph_store = GraphStoreFactory.for_graph_store(falkordb_connection_info)\n",
    "\n",
    "# Optional: confirm initialization\n",
    "print(f\"FalkorDB GraphStore initialized: {graph_store}\")"
   ],
   "id": "bc44bb4a9d0346db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalkorDB GraphStore initialized: log_formatting=RedactedGraphQueryLogFormatting() tenant_id=TenantId(value=None) endpoint_url='localhost:6379' database='graphrag' username=None password=None ssl=False\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check Bedrock Access",
   "id": "3c30faaf172372fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:11.952059Z",
     "start_time": "2025-04-26T21:52:10.291806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test Bedrock Access\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "def test_bedrock_access():\n",
    "    try:\n",
    "        # Create a session using the padmin profile\n",
    "        session = boto3.Session(profile_name='padmin')\n",
    "\n",
    "        # Create Bedrock clients\n",
    "        bedrock = session.client('bedrock', region_name='us-east-1')\n",
    "        bedrock_runtime = session.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "        # Test 1: List available models\n",
    "        print(\"Testing listing available models...\")\n",
    "        response = bedrock.list_foundation_models()\n",
    "        print(\"Available models:\")\n",
    "        for model in response['modelSummaries']:\n",
    "            print(f\"- {model['modelId']}\")\n",
    "\n",
    "        # Test 2: Try to invoke Titan model\n",
    "        print(\"\\nTesting model invocation with Titan...\")\n",
    "\n",
    "        # Titan-specific prompt format\n",
    "        test_prompt = {\n",
    "            \"inputText\": \"Say hello!\",\n",
    "            \"textGenerationConfig\": {\n",
    "                \"maxTokenCount\": 100,\n",
    "                \"temperature\": 0,\n",
    "                \"topP\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "        model_id = \"amazon.titan-text-express-v1\"  # Using Titan Express model\n",
    "\n",
    "        try:\n",
    "            print(f\"Testing model: {model_id}\")\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                modelId=model_id,\n",
    "                contentType='application/json',\n",
    "                accept='application/json',\n",
    "                body=json.dumps(test_prompt)\n",
    "            )\n",
    "\n",
    "            # Parse and log the response\n",
    "            response_body = json.loads(response['body'].read())\n",
    "            print(f\"Successfully invoked {model_id}\")\n",
    "            print(f\"Response: {response_body['results'][0]['outputText']}\")\n",
    "\n",
    "        except ClientError as e:\n",
    "            error_code = e.response.get('Error', {}).get('Code', 'Unknown')\n",
    "            error_message = e.response.get('Error', {}).get('Message', 'Unknown error')\n",
    "            print(f\"Failed to invoke {model_id}: {error_code} - {error_message}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Print AWS profile and region being used\n",
    "    print(f\"Using AWS Profile: padmin\")\n",
    "    print(f\"Using Region: us-east-1\")\n",
    "\n",
    "    # Run the test\n",
    "    test_bedrock_access()"
   ],
   "id": "c16bb7e9be92a4ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AWS Profile: padmin\n",
      "Using Region: us-east-1\n",
      "Testing listing available models...\n",
      "Available models:\n",
      "- amazon.titan-tg1-large\n",
      "- amazon.titan-image-generator-v1:0\n",
      "- amazon.titan-image-generator-v1\n",
      "- amazon.titan-image-generator-v2:0\n",
      "- amazon.titan-text-premier-v1:0\n",
      "- amazon.nova-pro-v1:0:24k\n",
      "- amazon.nova-pro-v1:0:300k\n",
      "- amazon.nova-pro-v1:0\n",
      "- amazon.nova-lite-v1:0:24k\n",
      "- amazon.nova-lite-v1:0:300k\n",
      "- amazon.nova-lite-v1:0\n",
      "- amazon.nova-canvas-v1:0\n",
      "- amazon.nova-reel-v1:0\n",
      "- amazon.nova-reel-v1:1\n",
      "- amazon.nova-micro-v1:0:24k\n",
      "- amazon.nova-micro-v1:0:128k\n",
      "- amazon.nova-micro-v1:0\n",
      "- amazon.nova-sonic-v1:0\n",
      "- amazon.titan-embed-g1-text-02\n",
      "- amazon.titan-text-lite-v1:0:4k\n",
      "- amazon.titan-text-lite-v1\n",
      "- amazon.titan-text-express-v1:0:8k\n",
      "- amazon.titan-text-express-v1\n",
      "- amazon.titan-embed-text-v1:2:8k\n",
      "- amazon.titan-embed-text-v1\n",
      "- amazon.titan-embed-text-v2:0:8k\n",
      "- amazon.titan-embed-text-v2:0\n",
      "- amazon.titan-embed-image-v1:0\n",
      "- amazon.titan-embed-image-v1\n",
      "- stability.stable-diffusion-xl-v1:0\n",
      "- stability.stable-diffusion-xl-v1\n",
      "- ai21.jamba-instruct-v1:0\n",
      "- ai21.jamba-1-5-large-v1:0\n",
      "- ai21.jamba-1-5-mini-v1:0\n",
      "- anthropic.claude-instant-v1:2:100k\n",
      "- anthropic.claude-instant-v1\n",
      "- anthropic.claude-v2:0:18k\n",
      "- anthropic.claude-v2:0:100k\n",
      "- anthropic.claude-v2:1:18k\n",
      "- anthropic.claude-v2:1:200k\n",
      "- anthropic.claude-v2:1\n",
      "- anthropic.claude-v2\n",
      "- anthropic.claude-3-sonnet-20240229-v1:0:28k\n",
      "- anthropic.claude-3-sonnet-20240229-v1:0:200k\n",
      "- anthropic.claude-3-sonnet-20240229-v1:0\n",
      "- anthropic.claude-3-haiku-20240307-v1:0:48k\n",
      "- anthropic.claude-3-haiku-20240307-v1:0:200k\n",
      "- anthropic.claude-3-haiku-20240307-v1:0\n",
      "- anthropic.claude-3-opus-20240229-v1:0:12k\n",
      "- anthropic.claude-3-opus-20240229-v1:0:28k\n",
      "- anthropic.claude-3-opus-20240229-v1:0:200k\n",
      "- anthropic.claude-3-opus-20240229-v1:0\n",
      "- anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "- anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "- anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "- anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "- cohere.command-text-v14:7:4k\n",
      "- cohere.command-text-v14\n",
      "- cohere.command-r-v1:0\n",
      "- cohere.command-r-plus-v1:0\n",
      "- cohere.command-light-text-v14:7:4k\n",
      "- cohere.command-light-text-v14\n",
      "- cohere.embed-english-v3:0:512\n",
      "- cohere.embed-english-v3\n",
      "- cohere.embed-multilingual-v3:0:512\n",
      "- cohere.embed-multilingual-v3\n",
      "- deepseek.r1-v1:0\n",
      "- meta.llama3-8b-instruct-v1:0\n",
      "- meta.llama3-70b-instruct-v1:0\n",
      "- meta.llama3-1-8b-instruct-v1:0\n",
      "- meta.llama3-1-70b-instruct-v1:0\n",
      "- meta.llama3-2-11b-instruct-v1:0\n",
      "- meta.llama3-2-90b-instruct-v1:0\n",
      "- meta.llama3-2-1b-instruct-v1:0\n",
      "- meta.llama3-2-3b-instruct-v1:0\n",
      "- meta.llama3-3-70b-instruct-v1:0\n",
      "- mistral.mistral-7b-instruct-v0:2\n",
      "- mistral.mixtral-8x7b-instruct-v0:1\n",
      "- mistral.mistral-large-2402-v1:0\n",
      "- mistral.mistral-small-2402-v1:0\n",
      "- mistral.pixtral-large-2502-v1:0\n",
      "\n",
      "Testing model invocation with Titan...\n",
      "Testing model: amazon.titan-text-express-v1\n",
      "Successfully invoked amazon.titan-text-express-v1\n",
      "Response: \n",
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inspect BedrockConverse",
   "id": "9f7bc63ed3953121"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:11.958300Z",
     "start_time": "2025-04-26T21:52:11.956712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inspect BedrockConverse\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "import inspect\n",
    "\n",
    "# Print the signature of the BedrockConverse constructor\n",
    "print(inspect.signature(BedrockConverse.__init__))"
   ],
   "id": "f391d9b143f08a42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, model: str, temperature: float = 0.1, max_tokens: Optional[int] = 512, profile_name: Optional[str] = None, aws_access_key_id: Optional[str] = None, aws_secret_access_key: Optional[str] = None, aws_session_token: Optional[str] = None, region_name: Optional[str] = None, botocore_session: Optional[Any] = None, client: Optional[Any] = None, timeout: Optional[float] = 60.0, max_retries: Optional[int] = 10, botocore_config: Optional[Any] = None, additional_kwargs: Optional[Dict[str, Any]] = None, callback_manager: Optional[llama_index.core.callbacks.base.CallbackManager] = None, system_prompt: Optional[str] = None, messages_to_prompt: Optional[Callable[[Sequence[llama_index.core.base.llms.types.ChatMessage]], str]] = None, completion_to_prompt: Optional[Callable[[str], str]] = None, pydantic_program_mode: llama_index.core.types.PydanticProgramMode = <PydanticProgramMode.DEFAULT: 'default'>, output_parser: Optional[llama_index.core.types.BaseOutputParser] = None, guardrail_identifier: Optional[str] = None, guardrail_version: Optional[str] = None, trace: Optional[str] = None) -> None\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inspect PostGreSQL DB",
   "id": "c4f208392b08ed7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:12.006975Z",
     "start_time": "2025-04-26T21:52:12.001947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Please verify that 'vector' is installed\n",
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"graphrag_db\",\n",
    "    user=\"graphrag\",\n",
    "    password=\"graphragpass\",\n",
    "    host=\"localhost\",  # or \"host.docker.internal\" if you're outside Docker\n",
    "    port=5432\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT extname FROM pg_extension;\")\n",
    "print(\"Extensions installed:\", cur.fetchall())"
   ],
   "id": "ac152737b0b2d9c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extensions installed: [('plpgsql',), ('vector',)]\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CPU",
   "id": "7ca431d25298a032"
  },
  {
   "cell_type": "markdown",
   "id": "3dfff49b",
   "metadata": {},
   "source": [
    "## Extract\n",
    "\n",
    "See [Run the extract and build stages separately](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/indexing.md#run-the-extract-and-build-stages-separately)"
   ]
  },
  {
   "cell_type": "code",
   "id": "79b6c7bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:13.044092Z",
     "start_time": "2025-04-26T21:52:12.046610Z"
    }
   },
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import FileBasedDocs\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "extracted_docs = FileBasedDocs(\n",
    "    docs_directory='extracted'\n",
    ")\n",
    "\n",
    "checkpoint = Checkpoint('extraction-checkpoint')\n",
    "\n",
    "\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store,\n",
    "    vector_store\n",
    ")\n",
    "\n",
    "doc_urls = [\n",
    "    'https://docs.aws.amazon.com/neptune/latest/userguide/intro.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-features.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-vs-neptune-database.html'\n",
    "]\n",
    "\n",
    "docs = SimpleWebPageReader(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url:{'url': url}\n",
    ").load_data(doc_urls)\n",
    "\n",
    "graph_index.extract(docs, handler=extracted_docs, checkpoint=checkpoint, show_progress=True)\n",
    "\n",
    "collection_id = extracted_docs.collection_id\n",
    "\n",
    "print('Extraction complete')\n",
    "print(f'collection_id: {collection_id}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot find .env file\n",
      "2025-04-26 17:52:12:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting propositions [nodes: 0, num_workers: 4]: 0it [00:00, ?it/s]\n",
      "Extracting topics [nodes: 0, num_workers: 4]: 0it [00:00, ?it/s]\n",
      "Extracting propositions [nodes: 0, num_workers: 4]: 0it [00:00, ?it/s]\n",
      "Extracting topics [nodes: 0, num_workers: 4]: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete\n",
      "collection_id: 20250426-175212\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "40c3f5e1",
   "metadata": {},
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "id": "eaa952bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:13.055600Z",
     "start_time": "2025-04-26T21:52:13.049275Z"
    }
   },
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import FileBasedDocs\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "docs = FileBasedDocs(\n",
    "    docs_directory='extracted',\n",
    "    collection_id=collection_id\n",
    ")\n",
    "checkpoint = Checkpoint('build-checkpoint')\n",
    "\n",
    "\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store, \n",
    "    vector_store\n",
    ")\n",
    "\n",
    "graph_index.build(docs, checkpoint=checkpoint, show_progress=True)\n",
    "\n",
    "print('Build complete')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot find .env file\n",
      "Build complete\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GPU",
   "id": "8b2dba0443b22c32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extract\n",
    "\n",
    "See [Run the extract and build stages separately](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/indexing.md#run-the-extract-and-build-stages-separately)"
   ],
   "id": "99a89254e40d01ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:14.823208Z",
     "start_time": "2025-04-26T21:52:13.096240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import nest_asyncio\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config, GraphRAGConfig\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory, VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import FileBasedDocs\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "nest_asyncio.apply()\n",
    "set_logging_config('INFO')\n",
    "\n",
    "# Inject HuggingFace embedding model using GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "GraphRAGConfig.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Setup extraction output\n",
    "extracted_docs = FileBasedDocs(docs_directory='extracted')\n",
    "checkpoint = Checkpoint('extraction-checkpoint')\n",
    "\n",
    "# Initialize GraphRAG components\n",
    "graph_index = LexicalGraphIndex(graph_store, vector_store)\n",
    "\n",
    "# Load documents\n",
    "doc_urls = [\n",
    "    'https://docs.aws.amazon.com/neptune/latest/userguide/intro.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-features.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-vs-neptune-database.html'\n",
    "]\n",
    "\n",
    "docs = SimpleWebPageReader(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url: {'url': url}\n",
    ").load_data(doc_urls)\n",
    "\n",
    "# Extract to intermediate checkpoint with GPU-powered embedding\n",
    "graph_index.extract(docs, handler=extracted_docs, checkpoint=checkpoint, show_progress=True)\n",
    "\n",
    "collection_id = extracted_docs.collection_id\n",
    "print('Extraction complete')\n",
    "print(f'collection_id: {collection_id}')\n"
   ],
   "id": "3fbfbf6583c1d92f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot find .env file\n",
      "Using device: cuda\n",
      "2025-04-26 17:52:13:INFO:s.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2025-04-26 17:52:13:INFO:s.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2025-04-26 17:52:14:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting propositions [nodes: 0, num_workers: 4]: 0it [00:00, ?it/s]\n",
      "Extracting propositions [nodes: 0, num_workers: 4]: 0it [00:00, ?it/s]\n",
      "Extracting topics [nodes: 0, num_workers: 4]: 0it [00:00, ?it/s]\n",
      "Extracting topics [nodes: 0, num_workers: 4]: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete\n",
      "collection_id: 20250426-175213\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build",
   "id": "a5d60061b4b51b6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:52:15.471088Z",
     "start_time": "2025-04-26T21:52:14.830767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import nest_asyncio\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config, GraphRAGConfig\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory, VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import FileBasedDocs\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "nest_asyncio.apply()\n",
    "set_logging_config('INFO')\n",
    "\n",
    "# Force GPU embedding model again before building\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "GraphRAGConfig.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load extracted docs\n",
    "docs = FileBasedDocs(\n",
    "    docs_directory='extracted',\n",
    "    collection_id=collection_id  # ⬅️ From previous notebook\n",
    ")\n",
    "\n",
    "checkpoint = Checkpoint('build-checkpoint')\n",
    "\n",
    "# Initialize and build\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store,\n",
    "    vector_store\n",
    ")\n",
    "\n",
    "graph_index.build(docs, checkpoint=checkpoint, show_progress=True)\n",
    "\n",
    "print('Build complete')\n"
   ],
   "id": "cb56047bc8529a17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot find .env file\n",
      "Using device: cuda\n",
      "2025-04-26 17:52:14:INFO:s.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2025-04-26 17:52:15:INFO:s.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "Build complete\n"
     ]
    }
   ],
   "execution_count": 57
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
