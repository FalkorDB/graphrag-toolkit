{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb1535a",
   "metadata": {},
   "source": "# 01 - Combined Extract and Build for local development"
  },
  {
   "cell_type": "markdown",
   "id": "e3f529c1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven't already, install the toolkit and dependencies using the [Setup](./00-Setup.ipynb) notebook."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup local development",
   "id": "cc035f82f4b8307a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check GPU support on Ubuntu 24.04 LTS",
   "id": "5f16604fdd69fc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:38:42.557535Z",
     "start_time": "2025-04-28T21:38:41.741914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from sympy.codegen.fnodes import dimension\n",
    "\n",
    "# Force inject paths (adapt to your system if different)\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "os.environ[\"PATH\"] += \":/usr/local/cuda/bin\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"EMBEDDINGS_MODEL\"] = \"cohere.embed-english-v3\"\n",
    "os.environ[\"EMBEDDINGS_DIMENSIONS\"] = \"1024\"\n",
    "\n",
    "# Re-check CUDA\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Version (Torch):\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"GPU still not detected inside notebook\")"
   ],
   "id": "277131fbcf61d064",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3060\n",
      "CUDA Version (Torch): 12.6\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup AWS Profile",
   "id": "1ac2cb7793453bec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:38:57.093410Z",
     "start_time": "2025-04-28T21:38:55.838309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure AWS Profile and Region\n",
    "from graphrag_toolkit.lexical_graph import GraphRAGConfig\n",
    "\n",
    "# Assign profile and region to GraphRAGConfig\n",
    "GraphRAGConfig.aws_profile = \"padmin\" #Optional, use if using AWS SSO\n",
    "GraphRAGConfig.aws_region = \"us-east-1\""
   ],
   "id": "e74fedb0db3da7c0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup model",
   "id": "34c7e8fe3e3e393"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:39:06.291719Z",
     "start_time": "2025-04-28T21:39:06.289582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set Claude model via Bedrock\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\""
   ],
   "id": "ee3ae5134ebf87a0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup BedrockConverse",
   "id": "111831ea2ea09703"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:39:16.799577Z",
     "start_time": "2025-04-28T21:39:16.665663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure BedrockConverse\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "\n",
    "try:\n",
    "    GraphRAGConfig.extraction_llm = BedrockConverse.from_json(f'''\n",
    "    {{\n",
    "        \"model\": \"{model_id}\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"profile_name\": \"{GraphRAGConfig.aws_profile}\",\n",
    "        \"region_name\": \"{GraphRAGConfig.aws_region}\"\n",
    "    }}\n",
    "    ''')\n",
    "    print(f\"Successfully configured Bedrock model: {model_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize BedrockConverse: {str(e)}\")\n",
    "    raise"
   ],
   "id": "5f96c802bf3ddca4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully configured Bedrock model: us.anthropic.claude-3-5-sonnet-20240620-v1:0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Display LLM Configuration",
   "id": "475a56517b5385ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:39:27.692540Z",
     "start_time": "2025-04-28T21:39:27.690707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display LLM configuration\n",
    "llm = GraphRAGConfig.extraction_llm\n",
    "print(\"LLM class:\", llm.__class__.__name__)\n",
    "print(\"Model ID:\", llm.model)\n",
    "print(\"Temperature:\", llm.temperature)\n",
    "print(\"Max tokens:\", llm.max_tokens)\n",
    "print(\"Profile:\", getattr(llm, 'profile_name', None))\n",
    "print(\"Region:\", getattr(llm, 'region_name', None))"
   ],
   "id": "2e776f0b240e25e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM class: BedrockConverse\n",
      "Model ID: us.anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "Temperature: 0.0\n",
      "Max tokens: 4096\n",
      "Profile: padmin\n",
      "Region: us-east-1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Display GraphRag Config",
   "id": "4606ea8a48843dc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:30:11.091006Z",
     "start_time": "2025-04-28T21:30:11.088480Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"GraphRAGConfig: {GraphRAGConfig}\")\n",
   "id": "c2b9c80d7e0b68fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphRAGConfig: _GraphRAGConfig(_aws_profile='padmin', _aws_region='us-east-1', _aws_clients={}, _extraction_llm=BedrockConverse(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x72d8834949b0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x72d72df916c0>, completion_to_prompt=<function default_completion_to_prompt at 0x72d72e02cf40>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='us.anthropic.claude-3-5-sonnet-20240620-v1:0', temperature=0.0, max_tokens=4096, profile_name='padmin', aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None, region_name='us-east-1', botocore_session=None, botocore_config=None, max_retries=10, timeout=60.0, guardrail_identifier=None, guardrail_version=None, trace=None, additional_kwargs={}), _response_llm=None, _embed_model=None, _embed_dimensions=None, _reranking_model=None, _extraction_num_workers=None, _extraction_num_threads_per_worker=None, _extraction_batch_size=None, _build_num_workers=None, _build_batch_size=None, _build_batch_write_size=None, _batch_writes_enabled=None, _include_domain_labels=None, _enable_cache=None)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup connection with PostgreSQL",
   "id": "494337e55f348e18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:39:45.297550Z",
     "start_time": "2025-04-28T21:39:45.237593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to PostgreSQL Vector Store\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "\n",
    "# PostgreSQL connection string\n",
    "postgre_connection_info = 'postgresql://graphrag:graphragpass@localhost:5432/graphrag_db'\n",
    "\n",
    "# Instantiate vector store using factory\n",
    "vector_store = VectorStoreFactory.for_vector_store(postgre_connection_info)\n",
    "\n",
    "# Optional: confirm\n",
    "print(f\"Vector store initialized: {vector_store}\")"
   ],
   "id": "f7b35557a9c02123",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized: indexes={'chunk': PGIndex(index_name='chunk', tenant_id=TenantId(value=None), writeable=True, database='graphrag_db', schema_name='graphrag', host='localhost', port=5432, username='graphrag', password='graphragpass', dimensions=1024, embed_model=BedrockEmbedding(model_name='cohere.embed-english-v3', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7185b40dc380>, num_workers=None, profile_name='padmin', aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None, region_name='us-east-1', botocore_session=None, botocore_config=None, max_retries=10, timeout=60.0, additional_kwargs={}), enable_iam_db_auth=False, initialized=False), 'statement': PGIndex(index_name='statement', tenant_id=TenantId(value=None), writeable=True, database='graphrag_db', schema_name='graphrag', host='localhost', port=5432, username='graphrag', password='graphragpass', dimensions=1024, embed_model=BedrockEmbedding(model_name='cohere.embed-english-v3', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7185b40dc380>, num_workers=None, profile_name='padmin', aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None, region_name='us-east-1', botocore_session=None, botocore_config=None, max_retries=10, timeout=60.0, additional_kwargs={}), enable_iam_db_auth=False, initialized=False)}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup connection with FalkorDB",
   "id": "e95f3bb922991a87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:30:16.525473Z",
     "start_time": "2025-04-28T21:30:14.786387Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.4.0.zip#subdirectory=lexical-graph-contrib/falkordb",
   "id": "4523416a2038ed24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.4.0.zip#subdirectory=lexical-graph-contrib/falkordb\r\n",
      "  Using cached https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.4.0.zip\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: FalkorDB in /home/evanerwee/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages (from graphrag-toolkit-lexical-graph-falkordb==1.0.0) (1.1.1)\r\n",
      "Requirement already satisfied: redis in /home/evanerwee/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages (from graphrag-toolkit-lexical-graph-falkordb==1.0.0) (5.2.1)\r\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:39:58.732178Z",
     "start_time": "2025-04-28T21:39:58.703792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to FalkorDB Graph Store\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "# Connection string for FalkorDB\n",
    "falkordb_connection_info = 'falkordb://localhost:6379'\n",
    "\n",
    "# Register the FalkorDB backend with the factory\n",
    "GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "# Instantiate a graph store using the factory\n",
    "graph_store = GraphStoreFactory.for_graph_store(falkordb_connection_info)\n",
    "\n",
    "# Optional: confirm initialization\n",
    "print(f\"FalkorDB GraphStore initialized: {graph_store}\")"
   ],
   "id": "7a41e5712020eaf4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalkorDB GraphStore initialized: log_formatting=RedactedGraphQueryLogFormatting() tenant_id=TenantId(value=None) endpoint_url='localhost:6379' database='graphrag' username=None password=None ssl=False\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check Bedrock Access",
   "id": "6dd81d3e94a07fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:40:10.418361Z",
     "start_time": "2025-04-28T21:40:08.452439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test Bedrock Access\n",
    "\n",
    "import boto3\n",
    "import logging\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "def test_bedrock_access():\n",
    "    try:\n",
    "        # Create a session using the padmin profile\n",
    "        session = boto3.Session(profile_name='padmin')\n",
    "\n",
    "        # Create Bedrock clients\n",
    "        bedrock = session.client('bedrock', region_name='us-east-1')\n",
    "        bedrock_runtime = session.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "        # Test 1: List available models\n",
    "        print(\"Testing listing available models...\")\n",
    "        response = bedrock.list_foundation_models()\n",
    "        print(\"Available models:\")\n",
    "        for model in response['modelSummaries']:\n",
    "            print(f\"- {model['modelId']}\")\n",
    "\n",
    "        # Test 2: Try to invoke Titan model\n",
    "        print(\"\\nTesting model invocation with Titan...\")\n",
    "\n",
    "        # Titan-specific prompt format\n",
    "        test_prompt = {\n",
    "            \"inputText\": \"Say hello!\",\n",
    "            \"textGenerationConfig\": {\n",
    "                \"maxTokenCount\": 100,\n",
    "                \"temperature\": 0,\n",
    "                \"topP\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "        model_id = \"amazon.titan-text-express-v1\"  # Using Titan Express model\n",
    "\n",
    "        try:\n",
    "            print(f\"Testing model: {model_id}\")\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                modelId=model_id,\n",
    "                contentType='application/json',\n",
    "                accept='application/json',\n",
    "                body=json.dumps(test_prompt)\n",
    "            )\n",
    "\n",
    "            # Parse and log the response\n",
    "            response_body = json.loads(response['body'].read())\n",
    "            print(f\"Successfully invoked {model_id}\")\n",
    "            print(f\"Response: {response_body['results'][0]['outputText']}\")\n",
    "\n",
    "        except ClientError as e:\n",
    "            error_code = e.response.get('Error', {}).get('Code', 'Unknown')\n",
    "            error_message = e.response.get('Error', {}).get('Message', 'Unknown error')\n",
    "            print(f\"Failed to invoke {model_id}: {error_code} - {error_message}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Print AWS profile and region being used\n",
    "    print(f\"Using AWS Profile: padmin\")\n",
    "    print(f\"Using Region: us-east-1\")\n",
    "\n",
    "    # Run the test\n",
    "    test_bedrock_access()\n"
   ],
   "id": "1af9d291f0ba0acf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AWS Profile: padmin\n",
      "Using Region: us-east-1\n",
      "Testing listing available models...\n",
      "Available models:\n",
      "- amazon.titan-tg1-large\n",
      "- amazon.titan-image-generator-v1:0\n",
      "- amazon.titan-image-generator-v1\n",
      "- amazon.titan-image-generator-v2:0\n",
      "- amazon.titan-text-premier-v1:0\n",
      "- amazon.nova-pro-v1:0:24k\n",
      "- amazon.nova-pro-v1:0:300k\n",
      "- amazon.nova-pro-v1:0\n",
      "- amazon.nova-lite-v1:0:24k\n",
      "- amazon.nova-lite-v1:0:300k\n",
      "- amazon.nova-lite-v1:0\n",
      "- amazon.nova-canvas-v1:0\n",
      "- amazon.nova-reel-v1:0\n",
      "- amazon.nova-reel-v1:1\n",
      "- amazon.nova-micro-v1:0:24k\n",
      "- amazon.nova-micro-v1:0:128k\n",
      "- amazon.nova-micro-v1:0\n",
      "- amazon.nova-sonic-v1:0\n",
      "- amazon.titan-embed-g1-text-02\n",
      "- amazon.titan-text-lite-v1:0:4k\n",
      "- amazon.titan-text-lite-v1\n",
      "- amazon.titan-text-express-v1:0:8k\n",
      "- amazon.titan-text-express-v1\n",
      "- amazon.titan-embed-text-v1:2:8k\n",
      "- amazon.titan-embed-text-v1\n",
      "- amazon.titan-embed-text-v2:0:8k\n",
      "- amazon.titan-embed-text-v2:0\n",
      "- amazon.titan-embed-image-v1:0\n",
      "- amazon.titan-embed-image-v1\n",
      "- stability.stable-diffusion-xl-v1:0\n",
      "- stability.stable-diffusion-xl-v1\n",
      "- ai21.jamba-instruct-v1:0\n",
      "- ai21.jamba-1-5-large-v1:0\n",
      "- ai21.jamba-1-5-mini-v1:0\n",
      "- anthropic.claude-instant-v1:2:100k\n",
      "- anthropic.claude-instant-v1\n",
      "- anthropic.claude-v2:0:18k\n",
      "- anthropic.claude-v2:0:100k\n",
      "- anthropic.claude-v2:1:18k\n",
      "- anthropic.claude-v2:1:200k\n",
      "- anthropic.claude-v2:1\n",
      "- anthropic.claude-v2\n",
      "- anthropic.claude-3-sonnet-20240229-v1:0:28k\n",
      "- anthropic.claude-3-sonnet-20240229-v1:0:200k\n",
      "- anthropic.claude-3-sonnet-20240229-v1:0\n",
      "- anthropic.claude-3-haiku-20240307-v1:0:48k\n",
      "- anthropic.claude-3-haiku-20240307-v1:0:200k\n",
      "- anthropic.claude-3-haiku-20240307-v1:0\n",
      "- anthropic.claude-3-opus-20240229-v1:0:12k\n",
      "- anthropic.claude-3-opus-20240229-v1:0:28k\n",
      "- anthropic.claude-3-opus-20240229-v1:0:200k\n",
      "- anthropic.claude-3-opus-20240229-v1:0\n",
      "- anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "- anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "- anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "- anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "- cohere.command-text-v14:7:4k\n",
      "- cohere.command-text-v14\n",
      "- cohere.command-r-v1:0\n",
      "- cohere.command-r-plus-v1:0\n",
      "- cohere.command-light-text-v14:7:4k\n",
      "- cohere.command-light-text-v14\n",
      "- cohere.embed-english-v3:0:512\n",
      "- cohere.embed-english-v3\n",
      "- cohere.embed-multilingual-v3:0:512\n",
      "- cohere.embed-multilingual-v3\n",
      "- deepseek.r1-v1:0\n",
      "- meta.llama3-8b-instruct-v1:0\n",
      "- meta.llama3-70b-instruct-v1:0\n",
      "- meta.llama3-1-8b-instruct-v1:0\n",
      "- meta.llama3-1-70b-instruct-v1:0\n",
      "- meta.llama3-2-11b-instruct-v1:0\n",
      "- meta.llama3-2-90b-instruct-v1:0\n",
      "- meta.llama3-2-1b-instruct-v1:0\n",
      "- meta.llama3-2-3b-instruct-v1:0\n",
      "- meta.llama3-3-70b-instruct-v1:0\n",
      "- mistral.mistral-7b-instruct-v0:2\n",
      "- mistral.mixtral-8x7b-instruct-v0:1\n",
      "- mistral.mistral-large-2402-v1:0\n",
      "- mistral.mistral-small-2402-v1:0\n",
      "- mistral.pixtral-large-2502-v1:0\n",
      "\n",
      "Testing model invocation with Titan...\n",
      "Testing model: amazon.titan-text-express-v1\n",
      "Successfully invoked amazon.titan-text-express-v1\n",
      "Response: \n",
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inspect BedrockConverse",
   "id": "85d36ae8bc33a6a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:30:25.145758Z",
     "start_time": "2025-04-28T21:30:25.143149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inspect BedrockConverse\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "import inspect\n",
    "\n",
    "# Print the signature of the BedrockConverse constructor\n",
    "print(inspect.signature(BedrockConverse.__init__))"
   ],
   "id": "6f93fe60d82f6975",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, model: str, temperature: float = 0.1, max_tokens: Optional[int] = 512, profile_name: Optional[str] = None, aws_access_key_id: Optional[str] = None, aws_secret_access_key: Optional[str] = None, aws_session_token: Optional[str] = None, region_name: Optional[str] = None, botocore_session: Optional[Any] = None, client: Optional[Any] = None, timeout: Optional[float] = 60.0, max_retries: Optional[int] = 10, botocore_config: Optional[Any] = None, additional_kwargs: Optional[Dict[str, Any]] = None, callback_manager: Optional[llama_index.core.callbacks.base.CallbackManager] = None, system_prompt: Optional[str] = None, messages_to_prompt: Optional[Callable[[Sequence[llama_index.core.base.llms.types.ChatMessage]], str]] = None, completion_to_prompt: Optional[Callable[[str], str]] = None, pydantic_program_mode: llama_index.core.types.PydanticProgramMode = <PydanticProgramMode.DEFAULT: 'default'>, output_parser: Optional[llama_index.core.types.BaseOutputParser] = None, guardrail_identifier: Optional[str] = None, guardrail_version: Optional[str] = None, trace: Optional[str] = None) -> None\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inspect PostGreSQL DB",
   "id": "ab72da9f403ff1e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:30:26.803993Z",
     "start_time": "2025-04-28T21:30:26.796936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Please verify that 'vector' is installed\n",
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"graphrag_db\",\n",
    "    user=\"graphrag\",\n",
    "    password=\"graphragpass\",\n",
    "    host=\"localhost\",  # or \"host.docker.internal\" if you're outside Docker\n",
    "    port=5432\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT extname FROM pg_extension;\")\n",
    "print(\"Extensions installed:\", cur.fetchall())\n"
   ],
   "id": "374ea7c03e6c5cdc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extensions installed: [('plpgsql',), ('vector',)]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Continous ingest\n",
    "\n",
    "See [Continous ingest](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/indexing.md#continous-ingest)."
   ],
   "id": "0aa94cc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CPU-Enabled Version of Your Script",
   "id": "df7985e1011159a6"
  },
  {
   "cell_type": "code",
   "id": "7ec68542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:44:15.751247Z",
     "start_time": "2025-04-28T21:40:46.972233Z"
    }
   },
   "source": [
    "# === Fully reset environment, reload variables ===\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import set_logging_config, GraphRAGConfig\n",
    "from graphrag_toolkit.lexical_graph.indexing.extract import (\n",
    "    ExtractionPipeline,\n",
    "    LLMPropositionExtractor,\n",
    "    TopicExtractor,\n",
    "    GraphScopedValueStore,\n",
    "    ScopedValueProvider\n",
    ")\n",
    "from graphrag_toolkit.lexical_graph.indexing.constants import PROPOSITIONS_KEY, DEFAULT_ENTITY_CLASSIFICATIONS\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import BuildPipeline, GraphConstruction, VectorIndexing\n",
    "from graphrag_toolkit.lexical_graph.indexing import sink\n",
    "from graphrag_toolkit.lexical_graph.tenant_id import TenantId\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "# Configuration\n",
    "set_logging_config('INFO')\n",
    "\n",
    "GraphRAGConfig.tenant_id = TenantId(\"dev123\")\n",
    "\n",
    "# Assume graph_store and vector_store already instantiated\n",
    "\n",
    "# === Setup Extraction Pipeline ===\n",
    "entity_classification_provider = ScopedValueProvider(\n",
    "    label='EntityClassification',\n",
    "    scoped_value_store=GraphScopedValueStore(graph_store=graph_store),\n",
    "    initial_scoped_values={\"default\": DEFAULT_ENTITY_CLASSIFICATIONS}\n",
    ")\n",
    "\n",
    "proposition_extractor = LLMPropositionExtractor()\n",
    "topic_extractor = TopicExtractor(\n",
    "    source_metadata_field=PROPOSITIONS_KEY,\n",
    "    entity_classification_provider=entity_classification_provider\n",
    ")\n",
    "\n",
    "extraction_pipeline = ExtractionPipeline.create(\n",
    "    components=[\n",
    "        proposition_extractor,  # critical!\n",
    "        topic_extractor\n",
    "    ],\n",
    "    num_workers=1,              # safe for Bedrock\n",
    "    batch_size=4,\n",
    "    checkpoint=None,\n",
    "    tenant_id=GraphRAGConfig.tenant_id,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# === Setup Build Pipeline ===\n",
    "graph_construction = GraphConstruction.for_graph_store(graph_store)\n",
    "vector_indexing = VectorIndexing.for_vector_store(vector_store)\n",
    "\n",
    "build_pipeline = BuildPipeline.create(\n",
    "    components=[\n",
    "        graph_construction,\n",
    "        vector_indexing\n",
    "    ],\n",
    "    num_workers=1,\n",
    "    batch_size=10,\n",
    "    batch_writes_enabled=True,\n",
    "    tenant_id=GraphRAGConfig.tenant_id,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# === Load documents ===\n",
    "doc_urls = [\n",
    "    'https://docs.aws.amazon.com/neptune/latest/userguide/intro.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-features.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-vs-neptune-database.html'\n",
    "]\n",
    "\n",
    "docs = SimpleWebPageReader(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url: {'url': url}\n",
    ").load_data(doc_urls)\n",
    "\n",
    "# === Run Extraction and Build ===\n",
    "docs | extraction_pipeline | build_pipeline | sink\n",
    "\n",
    "print('Extraction and Build Complete!')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 17:40:47:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting propositions [nodes: 4, num_workers: 4]: 100%|██████████| 4/4 [00:26<00:00,  6.57s/it]\n",
      "Extracting topics [nodes: 4, num_workers: 4]: 100%|██████████| 4/4 [02:58<00:00, 44.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 17:44:12:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 10, num_workers: 1, job_sizes: [343], batch_writes_enabled: True, batch_write_size: 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 343/343 [00:00<00:00, 103224.96it/s]\n",
      "Building vector index [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 343/343 [00:00<00:00, 652743.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extraction and Build Complete!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GPU-Enabled Version of Your Script",
   "id": "c7304b52300643b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:07:40.522650Z",
     "start_time": "2025-04-28T21:06:28.134339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fully reset environment, reload variables\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio (needed for async inside Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Imports\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, GraphRAGConfig, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.tenant_id import TenantId\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "# Set logging level\n",
    "set_logging_config('INFO')\n",
    "\n",
    "GraphRAGConfig.tenant_id = TenantId(\"dev123\")\n",
    "\n",
    "# Now create GraphStore and VectorStore normally\n",
    "# (you must have graph_store and vector_store already instantiated somewhere before this, or add it here)\n",
    "\n",
    "# Create LexicalGraphIndex WITH tenant ID\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store=graph_store,\n",
    "    vector_store=vector_store,\n",
    "    tenant_id=TenantId(\"dev123\")   # <-- add this line\n",
    ")\n",
    "\n",
    "# Load documents (URLs)\n",
    "doc_urls = [\n",
    "    'https://docs.aws.amazon.com/neptune/latest/userguide/intro.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-features.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-vs-neptune-database.html'\n",
    "]\n",
    "\n",
    "# Read documents\n",
    "docs = SimpleWebPageReader(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url: {'url': url}\n",
    ").load_data(doc_urls)\n",
    "\n",
    "# Extract and Build the graph\n",
    "graph_index.extract_and_build(docs, show_progress=True)\n",
    "\n",
    "print('Complete!')\n"
   ],
   "id": "601c0631009c6927",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot find .env file\n",
      "2025-04-28 17:06:29:WARNING:g.l.lexical_graph_index:TenantId has been set to non-default tenant id, but extraction will use default tenant id\n",
      "2025-04-28 17:06:29:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting propositions [nodes: 5, num_workers: 4]: 100%|██████████| 5/5 [00:11<00:00,  2.30s/it]t]\n",
      "Extracting propositions [nodes: 10, num_workers: 4]: 100%|██████████| 10/10 [00:19<00:00,  1.93s/it]\n",
      "Extracting topics [nodes: 5, num_workers: 4]: 100%|██████████| 5/5 [00:23<00:00,  4.62s/it]t]\n",
      "Extracting topics [nodes: 10, num_workers: 4]: 100%|██████████| 10/10 [00:50<00:00,  5.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 17:07:39:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 2, job_sizes: [462, 188], batch_writes_enabled: True, batch_write_size: 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-27:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit4/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit4/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit4/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit4/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/torch/multiprocessing/reductions.py\", line 180, in rebuild_cuda_tensor\n",
      "    torch.cuda._lazy_init()\n",
      "  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 358, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mBrokenProcessPool\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[60]\u001B[39m\u001B[32m, line 45\u001B[39m\n\u001B[32m     39\u001B[39m docs = SimpleWebPageReader(\n\u001B[32m     40\u001B[39m     html_to_text=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m     41\u001B[39m     metadata_fn=\u001B[38;5;28;01mlambda\u001B[39;00m url: {\u001B[33m'\u001B[39m\u001B[33murl\u001B[39m\u001B[33m'\u001B[39m: url}\n\u001B[32m     42\u001B[39m ).load_data(doc_urls)\n\u001B[32m     44\u001B[39m \u001B[38;5;66;03m# Extract and Build the graph\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m45\u001B[39m graph_index.extract_and_build(docs, show_progress=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     47\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mComplete!\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/lexical_graph_index.py:358\u001B[39m, in \u001B[36mLexicalGraphIndex.extract_and_build\u001B[39m\u001B[34m(self, nodes, handler, checkpoint, show_progress, **kwargs)\u001B[39m\n\u001B[32m    344\u001B[39m build_pipeline = BuildPipeline.create(\n\u001B[32m    345\u001B[39m     components=[\n\u001B[32m    346\u001B[39m         GraphConstruction.for_graph_store(\u001B[38;5;28mself\u001B[39m.graph_store),\n\u001B[32m   (...)\u001B[39m\u001B[32m    354\u001B[39m     **kwargs\n\u001B[32m    355\u001B[39m )\n\u001B[32m    357\u001B[39m sink_fn = sink \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m handler \u001B[38;5;28;01melse\u001B[39;00m Pipe(handler.accept)\n\u001B[32m--> \u001B[39m\u001B[32m358\u001B[39m nodes | extraction_pipeline | build_pipeline | sink_fn\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/pipe.py:42\u001B[39m, in \u001B[36mPipe.__ror__\u001B[39m\u001B[34m(self, other)\u001B[39m\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__ror__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.function(other)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/pipe.py:36\u001B[39m, in \u001B[36mPipe.__init__.<locals>.<lambda>\u001B[39m\u001B[34m(iterable, *args2, **kwargs2)\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, function, *args, **kwargs):\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m     \u001B[38;5;28mself\u001B[39m.function = \u001B[38;5;28;01mlambda\u001B[39;00m iterable, *args2, **kwargs2: function(\n\u001B[32m     37\u001B[39m         iterable, *args, *args2, **kwargs, **kwargs2\n\u001B[32m     38\u001B[39m     )\n\u001B[32m     39\u001B[39m     functools.update_wrapper(\u001B[38;5;28mself\u001B[39m, function)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/utils/pipeline_utils.py:16\u001B[39m, in \u001B[36m_sink.<locals>._sink_from\u001B[39m\u001B[34m(generator)\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_sink_from\u001B[39m(generator):\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m generator:\n\u001B[32m     17\u001B[39m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/build/build_pipeline.py:154\u001B[39m, in \u001B[36mBuildPipeline.build\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m    150\u001B[39m node_batches:List[List[BaseNode]] = \u001B[38;5;28mself\u001B[39m._to_node_batches(source_doc_batches)\n\u001B[32m    152\u001B[39m logger.info(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mRunning build pipeline [batch_size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.batch_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, num_workers: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.num_workers\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, job_sizes: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m[\u001B[38;5;28mlen\u001B[39m(b)\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mb\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mnode_batches]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, batch_writes_enabled: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.batch_writes_enabled\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, batch_write_size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.batch_write_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m]\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m output_nodes = run_pipeline(\n\u001B[32m    155\u001B[39m     \u001B[38;5;28mself\u001B[39m.inner_pipeline,\n\u001B[32m    156\u001B[39m     node_batches,\n\u001B[32m    157\u001B[39m     num_workers=\u001B[38;5;28mself\u001B[39m.num_workers,\n\u001B[32m    158\u001B[39m     batch_writes_enabled=\u001B[38;5;28mself\u001B[39m.batch_writes_enabled,\n\u001B[32m    159\u001B[39m     batch_size=\u001B[38;5;28mself\u001B[39m.batch_size,\n\u001B[32m    160\u001B[39m     batch_write_size=\u001B[38;5;28mself\u001B[39m.batch_write_size,\n\u001B[32m    161\u001B[39m     include_domain_labels=\u001B[38;5;28mself\u001B[39m.include_domain_labels,\n\u001B[32m    162\u001B[39m     **\u001B[38;5;28mself\u001B[39m.pipeline_kwargs\n\u001B[32m    163\u001B[39m )\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m output_nodes:\n\u001B[32m    166\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m node\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/utils/pipeline_utils.py:41\u001B[39m, in \u001B[36mrun_pipeline\u001B[39m\u001B[34m(pipeline, node_batches, cache_collection, in_place, num_workers, **kwargs)\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m ProcessPoolExecutor(max_workers=num_workers) \u001B[38;5;28;01mas\u001B[39;00m p:\n\u001B[32m     40\u001B[39m     processed_node_batches = p.map(transform, node_batches)\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     processed_nodes = \u001B[38;5;28msum\u001B[39m(processed_node_batches, start=cast(List[BaseNode], []))\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m processed_nodes\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/concurrent/futures/process.py:636\u001B[39m, in \u001B[36m_chain_from_iterable_of_lists\u001B[39m\u001B[34m(iterable)\u001B[39m\n\u001B[32m    630\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_chain_from_iterable_of_lists\u001B[39m(iterable):\n\u001B[32m    631\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    632\u001B[39m \u001B[33;03m    Specialized implementation of itertools.chain.from_iterable.\u001B[39;00m\n\u001B[32m    633\u001B[39m \u001B[33;03m    Each item in *iterable* should be a list.  This function is\u001B[39;00m\n\u001B[32m    634\u001B[39m \u001B[33;03m    careful not to keep references to yielded objects.\u001B[39;00m\n\u001B[32m    635\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m636\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m element \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[32m    637\u001B[39m         element.reverse()\n\u001B[32m    638\u001B[39m         \u001B[38;5;28;01mwhile\u001B[39;00m element:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/concurrent/futures/_base.py:619\u001B[39m, in \u001B[36mExecutor.map.<locals>.result_iterator\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m fs:\n\u001B[32m    617\u001B[39m     \u001B[38;5;66;03m# Careful not to keep a reference to the popped future\u001B[39;00m\n\u001B[32m    618\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m619\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m _result_or_cancel(fs.pop())\n\u001B[32m    620\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    621\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/concurrent/futures/_base.py:317\u001B[39m, in \u001B[36m_result_or_cancel\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    316\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m317\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m fut.result(timeout)\n\u001B[32m    318\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    319\u001B[39m         fut.cancel()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/concurrent/futures/_base.py:456\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    454\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[32m    455\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state == FINISHED:\n\u001B[32m--> \u001B[39m\u001B[32m456\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.__get_result()\n\u001B[32m    457\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    458\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/concurrent/futures/_base.py:401\u001B[39m, in \u001B[36mFuture.__get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    399\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception:\n\u001B[32m    400\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    403\u001B[39m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[32m    404\u001B[39m         \u001B[38;5;28mself\u001B[39m = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[31mBrokenProcessPool\u001B[39m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, GraphRAGConfig, set_logging_config\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from graphrag_toolkit.lexical_graph.tenant_id import TenantId\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "set_logging_config('INFO')\n",
    "\n",
    "# Prefer GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "GraphRAGConfig.tenant_id = TenantId(\"dev123\")\n",
    "\n",
    "# Configure embedding model to use GPU\n",
    "GraphRAGConfig.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"all-mpnet-base-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize GraphRAG index\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store,\n",
    "    vector_store,\n",
    "    tenant_id=GraphRAGConfig.tenant_id\n",
    ")\n",
    "\n",
    "# Load docs\n",
    "doc_urls = [\n",
    "    'https://docs.aws.amazon.com/neptune/latest/userguide/intro.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-features.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-vs-neptune-database.html'\n",
    "]\n",
    "\n",
    "docs = SimpleWebPageReader(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url: {'url': url}\n",
    ").load_data(doc_urls)\n",
    "\n",
    "# Run GraphRAG build\n",
    "graph_index.extract_and_build(docs, show_progress=True)\n",
    "\n",
    "print('Complete')\n"
   ],
   "id": "2749da80a7f3cdd0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
