{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "434fea4e",
   "metadata": {},
   "source": [
    "# 02 - Separate Extract and Build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb5cff",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven't already, install the toolkit and dependencies using the [Setup](./00-Setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfff49b",
   "metadata": {},
   "source": [
    "## Local extract to folder\n",
    "\n",
    "See [Run the extract and build stages separately](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/indexing.md#run-the-extract-and-build-stages-separately)"
   ]
  },
  {
   "cell_type": "code",
   "id": "79b6c7bd",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import FileBasedDocs\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "# Register the FalkorDB backend with the factory\n",
    "GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "extracted_docs = FileBasedDocs(\n",
    "    docs_directory='extracted'\n",
    ")\n",
    "\n",
    "checkpoint = Checkpoint('extraction-checkpoint')\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store, \n",
    "    vector_store\n",
    ")\n",
    "\n",
    "doc_urls = [\n",
    "    'https://docs.aws.amazon.com/neptune/latest/userguide/intro.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-features.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-vs-neptune-database.html'\n",
    "]\n",
    "\n",
    "docs = SimpleWebPageReader(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url:{'url': url}\n",
    ").load_data(doc_urls)\n",
    "\n",
    "graph_index.extract(docs, handler=extracted_docs, checkpoint=checkpoint, show_progress=True)\n",
    "\n",
    "collection_id = extracted_docs.collection_id\n",
    "\n",
    "print('Extraction complete')\n",
    "print(f'collection_id: {collection_id}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extraction to S3\n",
    "\n",
    "See [Run the extract and build stages separately](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/indexing.md#run-the-extract-and-build-stages-separately)"
   ],
   "id": "b5ed8c34f6f937fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import S3BasedDocs\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "# Register the FalkorDB backend with the factory\n",
    "GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "extracted_docs = S3BasedDocs(\n",
    "    region=os.environ['AWS_REGION'],\n",
    "    bucket_name=os.environ['S3_BUCKET_NAME'],\n",
    "    key_prefix='key_prefix',\n",
    "    collection_id='demo123'\n",
    ")\n",
    "\n",
    "checkpoint = Checkpoint('s3-extraction-checkpoint')\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store,\n",
    "    vector_store\n",
    ")\n",
    "\n",
    "doc_urls = [\n",
    "    'https://docs.aws.amazon.com/neptune/latest/userguide/intro.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-features.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-vs-neptune-database.html'\n",
    "]\n",
    "\n",
    "docs = SimpleWebPageReader(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url:{'url': url}\n",
    ").load_data(doc_urls)\n",
    "\n",
    "graph_index.extract(docs, handler=extracted_docs, checkpoint=checkpoint, show_progress=True)\n",
    "\n",
    "collection_id = extracted_docs.collection_id\n",
    "\n",
    "print('Extraction complete')\n",
    "print(f'collection_id: {collection_id}')"
   ],
   "id": "917e0345c606995f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using batch inference with the LexicalGraphIndex",
   "id": "87e80915f2101b04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ensure you have reviewed batch-extraction.md. For permission creation please see setup-bedrock-batch.md in lexical-graph-hybrid-dev/aws folder.",
   "id": "6e19fc7db1fd8061"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install PyMuPDF llama-index[pdf]\n",
   "id": "7d15700657e7cd06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import (\n",
    "    LexicalGraphIndex,\n",
    "    GraphRAGConfig,\n",
    "    IndexingConfig,\n",
    "    set_logging_config,\n",
    ")\n",
    "from graphrag_toolkit.lexical_graph.storage import (\n",
    "    GraphStoreFactory,\n",
    "    VectorStoreFactory,\n",
    ")\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.extract import BatchConfig\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "def batch_extract_and_load():\n",
    "    set_logging_config(\"INFO\")\n",
    "\n",
    "    # Register FalkorDB backend\n",
    "    GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "    # Set batch size\n",
    "    GraphRAGConfig.extraction_batch_size = int(os.environ.get(\"EXTRACTION_BATCH_SIZE\", 4))\n",
    "\n",
    "    # Configure batch S3 setup\n",
    "    batch_config = BatchConfig(\n",
    "        region=os.environ[\"AWS_REGION\"],\n",
    "        bucket_name=os.environ[\"S3_BATCH_BUCKET_NAME\"],\n",
    "        key_prefix=os.environ[\"BATCH_KEY_PREFIX_01\"],\n",
    "        role_arn=f'arn:aws:iam::{os.environ[\"AWS_ACCOUNT\"]}:role/{os.environ[\"BATCH_ROLE_NAME\"]}',\n",
    "\n",
    "    )\n",
    "\n",
    "    indexing_config = IndexingConfig(batch_config=batch_config)\n",
    "    checkpoint = Checkpoint(os.environ[\"BATCH_CHECKPOINT_01\"])\n",
    "\n",
    "    graph_store = GraphStoreFactory.for_graph_store(os.environ[\"GRAPH_STORE\"])\n",
    "    vector_store = VectorStoreFactory.for_vector_store(os.environ[\"VECTOR_STORE\"])\n",
    "\n",
    "    graph_index = LexicalGraphIndex(\n",
    "        graph_store,\n",
    "        vector_store,\n",
    "        indexing_config=indexing_config\n",
    "    )\n",
    "\n",
    "    # Use PyMuPDF for PDFs\n",
    "    file_extractor = {\n",
    "        \".pdf\": PyMuPDFReader()\n",
    "    }\n",
    "\n",
    "    reader = SimpleDirectoryReader(\n",
    "        input_dir=os.environ[\"BATCH_SOURCE_DIR\"],\n",
    "        file_extractor=file_extractor\n",
    "    )\n",
    "    docs = reader.load_data()\n",
    "\n",
    "    graph_index.extract(docs, checkpoint=checkpoint, show_progress=True)\n",
    "\n",
    "# Run the batch job\n",
    "batch_extract_and_load()\n"
   ],
   "id": "1354a15dc501953e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using batch inference with the LexicalGraphIndex. Writing to AWS S3 and DynamoDB",
   "id": "fc6568641b03d2da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:11:22.788129Z",
     "start_time": "2025-05-12T01:50:31.898507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import (\n",
    "    LexicalGraphIndex,\n",
    "    GraphRAGConfig,\n",
    "    IndexingConfig,\n",
    "    set_logging_config,\n",
    ")\n",
    "from graphrag_toolkit.lexical_graph.storage import (\n",
    "    GraphStoreFactory,\n",
    "    VectorStoreFactory,\n",
    ")\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.extract import BatchConfig\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "def batch_extract_and_load():\n",
    "    set_logging_config(\"INFO\")\n",
    "\n",
    "    # Register FalkorDB backend\n",
    "    GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "    # Initialize DynamoDB client\n",
    "    dynamodb = boto3.resource('dynamodb', region_name=os.environ['AWS_REGION'])\n",
    "    table = dynamodb.Table(os.environ.get('DYNAMODB_NAME'))\n",
    "\n",
    "    # Get collection_id from environment or generate a fallback\n",
    "    collection_id = os.environ.get('COLLECTION_ID', f\"batch-{int(time.time())}\")\n",
    "\n",
    "    # Check for existing record\n",
    "    try:\n",
    "        response = table.query(\n",
    "            KeyConditionExpression='collection_id = :id',\n",
    "            ExpressionAttributeValues={\n",
    "                ':id': collection_id\n",
    "            }\n",
    "        )\n",
    "        items = response.get('Items', [])\n",
    "        if items:\n",
    "            existing_status = items[0].get('status', 'UNKNOWN')\n",
    "            print(f\"Extraction for collection_id {collection_id} has already been run with status {existing_status}\")\n",
    "            # Check checkpoint state\n",
    "            checkpoint = Checkpoint(os.environ[\"BATCH_CHECKPOINT_01\"])\n",
    "            if existing_status == 'COMPLETED' and checkpoint.is_complete():\n",
    "                print(f\"Checkpoint {os.environ['BATCH_CHECKPOINT_01']} indicates extraction is complete. Skipping.\")\n",
    "                exit(0)\n",
    "            elif existing_status == 'IN_PROGRESS':\n",
    "                print(f\"Resuming extraction for collection_id {collection_id} with existing checkpoint.\")\n",
    "            else:\n",
    "                print(f\"Previous run failed or incomplete. Resuming with checkpoint.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying DynamoDB for existing record: {str(e)}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Set batch size\n",
    "    GraphRAGConfig.extraction_batch_size = int(os.environ.get(\"EXTRACTION_BATCH_SIZE\", 4))\n",
    "\n",
    "    # Configure batch S3 setup\n",
    "    batch_config = BatchConfig(\n",
    "        region=os.environ[\"AWS_REGION\"],\n",
    "        bucket_name=os.environ[\"S3_BATCH_BUCKET_NAME\"],\n",
    "        key_prefix=os.environ[\"BATCH_KEY_PREFIX_01\"],\n",
    "        role_arn=f'arn:aws:iam::{os.environ[\"AWS_ACCOUNT\"]}:role/{os.environ[\"BATCH_ROLE_NAME\"]}',\n",
    "    )\n",
    "\n",
    "    indexing_config = IndexingConfig(batch_config=batch_config)\n",
    "    checkpoint = Checkpoint(os.environ[\"BATCH_CHECKPOINT_01\"])\n",
    "\n",
    "    graph_store = GraphStoreFactory.for_graph_store(os.environ[\"GRAPH_STORE\"])\n",
    "    vector_store = VectorStoreFactory.for_vector_store(os.environ[\"VECTOR_STORE\"])\n",
    "\n",
    "    graph_index = LexicalGraphIndex(\n",
    "        graph_store,\n",
    "        vector_store,\n",
    "        indexing_config=indexing_config\n",
    "    )\n",
    "\n",
    "    # Use PyMuPDF for PDFs\n",
    "    file_extractor = {\n",
    "        \".pdf\": PyMuPDFReader()\n",
    "    }\n",
    "\n",
    "    reader = SimpleDirectoryReader(\n",
    "        input_dir=os.environ[\"BATCH_SOURCE_DIR\"],\n",
    "        file_extractor=file_extractor\n",
    "    )\n",
    "    docs = reader.load_data()\n",
    "\n",
    "    # Track start time\n",
    "    start_time = time.time()\n",
    "    status = 'IN_PROGRESS'\n",
    "    error_message = None\n",
    "    completion_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # Write initial record to DynamoDB if no record exists\n",
    "    try:\n",
    "        if not items:  # Only create if no record was found\n",
    "            table.put_item(Item={\n",
    "                'collection_id': collection_id,\n",
    "                'completion_date': completion_date,\n",
    "                'status': status,\n",
    "                'reader_type': 'PDF',\n",
    "                's3_bucket': os.environ['S3_BATCH_BUCKET_NAME'],\n",
    "                's3_key_prefix': os.environ['BATCH_KEY_PREFIX_01'],\n",
    "                'graph_store': os.environ['GRAPH_STORE'],\n",
    "                'vector_store': os.environ['VECTOR_STORE'],\n",
    "                'aws_region': os.environ['AWS_REGION'],\n",
    "                'start_time': datetime.fromtimestamp(start_time, tz=timezone.utc).isoformat(),\n",
    "                'duration': 0,\n",
    "                'document_count': len(docs),\n",
    "                'error_message': None,\n",
    "                'checkpoint': os.environ['BATCH_CHECKPOINT_01'],\n",
    "                'user_id': os.environ.get('USER_ID', 'unknown'),\n",
    "                'environment_variables': {\n",
    "                    'EXTRACTION_MODEL': os.environ.get('EXTRACTION_MODEL', ''),\n",
    "                    'EMBEDDINGS_MODEL': os.environ.get('EMBEDDINGS_MODEL', ''),\n",
    "                    'EMBEDDINGS_DIMENSIONS': os.environ.get('EMBEDDINGS_DIMENSIONS', ''),\n",
    "                    'EXTRACTION_BATCH_SIZE': os.environ.get('EXTRACTION_BATCH_SIZE', '4')\n",
    "                },\n",
    "                'reader_configuration': {\n",
    "                    'file_type': 'pdf',\n",
    "                    'reader_class': 'PyMuPDFReader'\n",
    "                }\n",
    "            })\n",
    "            print(f\"Initial DynamoDB record created for collection {collection_id}\")\n",
    "        else:\n",
    "            # Update existing record to IN_PROGRESS\n",
    "            table.put_item(Item={\n",
    "                'collection_id': collection_id,\n",
    "                'completion_date': completion_date,\n",
    "                'status': status,\n",
    "                'reader_type': 'PDF',\n",
    "                's3_bucket': os.environ['S3_BATCH_BUCKET_NAME'],\n",
    "                's3_key_prefix': os.environ['BATCH_KEY_PREFIX_01'],\n",
    "                'graph_store': os.environ['GRAPH_STORE'],\n",
    "                'vector_store': os.environ['VECTOR_STORE'],\n",
    "                'aws_region': os.environ['AWS_REGION'],\n",
    "                'start_time': datetime.fromtimestamp(start_time, tz=timezone.utc).isoformat(),\n",
    "                'duration': 0,\n",
    "                'document_count': len(docs),\n",
    "                'error_message': None,\n",
    "                'checkpoint': os.environ['BATCH_CHECKPOINT_01'],\n",
    "                'user_id': os.environ.get('USER_ID', 'unknown'),\n",
    "                'environment_variables': {\n",
    "                    'EXTRACTION_MODEL': os.environ.get('EXTRACTION_MODEL', ''),\n",
    "                    'EMBEDDINGS_MODEL': os.environ.get('EMBEDDINGS_MODEL', ''),\n",
    "                    'EMBEDDINGS_DIMENSIONS': os.environ.get('EMBEDDINGS_DIMENSIONS', ''),\n",
    "                    'EXTRACTION_BATCH_SIZE': os.environ.get('EXTRACTION_BATCH_SIZE', '4')\n",
    "                },\n",
    "                'reader_configuration': {\n",
    "                    'file_type': 'pdf',\n",
    "                    'reader_class': 'PyMuPDFReader'\n",
    "                }\n",
    "            })\n",
    "            print(f\"Updated DynamoDB record for collection {collection_id} to IN_PROGRESS\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating/updating initial DynamoDB record: {str(e)}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Perform extraction\n",
    "    try:\n",
    "        graph_index.extract(docs, checkpoint=checkpoint, show_progress=True)\n",
    "        status = 'COMPLETED'\n",
    "    except Exception as e:\n",
    "        status = 'FAILED'\n",
    "        error_message = str(e)\n",
    "        print(f\"Extraction failed: {str(e)}\")\n",
    "\n",
    "    # Update DynamoDB record with final status\n",
    "\n",
    "    duration = int(time.time() - start_time)\n",
    "\n",
    "    try:\n",
    "        table.put_item(Item={\n",
    "            'collection_id': collection_id,\n",
    "            'completion_date': completion_date,\n",
    "            'status': status,\n",
    "            'reader_type': 'PDF',\n",
    "            's3_bucket': os.environ['S3_BATCH_BUCKET_NAME'],\n",
    "            's3_key_prefix': os.environ['BATCH_KEY_PREFIX_01'],\n",
    "            'graph_store': os.environ['GRAPH_STORE'],\n",
    "            'vector_store': os.environ['VECTOR_STORE'],\n",
    "            'aws_region': os.environ['AWS_REGION'],\n",
    "            'start_time': datetime.fromtimestamp(start_time, tz=timezone.utc).isoformat(),\n",
    "            'duration': duration,\n",
    "            'document_count': len(docs),\n",
    "            'error_message': error_message,\n",
    "            'checkpoint': os.environ['BATCH_CHECKPOINT_01'],\n",
    "            'user_id': os.environ.get('USER_ID', 'unknown'),\n",
    "            'environment_variables': {\n",
    "                'EXTRACTION_MODEL': os.environ.get('EXTRACTION_MODEL', ''),\n",
    "                'EMBEDDINGS_MODEL': os.environ.get('EMBEDDINGS_MODEL', ''),\n",
    "                'EMBEDDINGS_DIMENSIONS': os.environ.get('EMBEDDINGS_DIMENSIONS', ''),\n",
    "                'EXTRACTION_BATCH_SIZE': os.environ.get('EXTRACTION_BATCH_SIZE', '4')\n",
    "            },\n",
    "            'reader_configuration': {\n",
    "                'file_type': 'pdf',\n",
    "                'reader_class': 'PyMuPDFReader'\n",
    "            }\n",
    "        })\n",
    "        print(f\"Stored collection {collection_id} in DynamoDB with status {status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing final DynamoDB record: {str(e)}\")\n",
    "        status = 'FAILED'\n",
    "        error_message = f\"Extraction: {error_message or 'Success'}, DynamoDB: {str(e)}\"\n",
    "        # Attempt to store a failure record\n",
    "        table.put_item(Item={\n",
    "            'collection_id': collection_id,\n",
    "            'completion_date': completion_date,\n",
    "            'status': status,\n",
    "            'reader_type': 'PDF',\n",
    "            's3_bucket': os.environ['S3_BATCH_BUCKET_NAME'],\n",
    "            's3_key_prefix': os.environ['BATCH_KEY_PREFIX_01'],\n",
    "            'graph_store': os.environ['GRAPH_STORE'],\n",
    "            'vector_store': os.environ['VECTOR_STORE'],\n",
    "            'aws_region': os.environ['AWS_REGION'],\n",
    "            'start_time': datetime.fromtimestamp(start_time, tz=timezone.utc).isoformat(),\n",
    "            'duration': duration,\n",
    "            'document_count': len(docs),\n",
    "            'error_message': error_message,\n",
    "            'checkpoint': os.environ['BATCH_CHECKPOINT_01'],\n",
    "            'user_id': os.environ.get('USER_ID', 'unknown'),\n",
    "            'environment_variables': {\n",
    "                'EXTRACTION_MODEL': os.environ.get('EXTRACTION_MODEL', ''),\n",
    "                'EMBEDDINGS_MODEL': os.environ.get('EMBEDDINGS_MODEL', ''),\n",
    "                'EMBEDDINGS_DIMENSIONS': os.environ.get('EMBEDDINGS_DIMENSIONS', ''),\n",
    "                'EXTRACTION_BATCH_SIZE': os.environ.get('EXTRACTION_BATCH_SIZE', '4')\n",
    "            },\n",
    "            'reader_configuration': {\n",
    "                'file_type': 'pdf',\n",
    "                'reader_class': 'PyMuPDFReader'\n",
    "            }\n",
    "        })\n",
    "\n",
    "    print('Extraction complete')\n",
    "    print(f'collection_id: {collection_id}')\n",
    "\n",
    "# Run the batch job\n",
    "batch_extract_and_load()"
   ],
   "id": "27e96c3ebc9faed4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DynamoDB record created for collection batch-1747014631\n",
      "2025-05-11 21:50:32:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 100, num_workers: 1]\n",
      "2025-05-11 21:50:34:INFO:g.l.i.u.batch_inference_utils:Created batch job [job_arn: arn:aws:bedrock:us-east-1:188967239867:model-invocation-job/fvlw1o4sj86o]\n",
      "2025-05-11 22:00:39:INFO:g.l.i.u.batch_inference_utils:Created batch job [job_arn: arn:aws:bedrock:us-east-1:188967239867:model-invocation-job/ibk53ab5i3ox]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [257], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [275], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [282], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [364], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [402], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [373], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [335], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [159], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [441], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [394], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [217], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [266], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [347], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [269], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [284], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [250], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [268], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [363], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [221], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [161], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [373], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:42:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [199], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:43:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [265], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:43:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [235], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:43:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [325], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:09:43:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 100, num_workers: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting propositions [nodes: 67, num_workers: 4]: 100%|██████████| 67/67 [00:24<00:00,  2.78it/s]\n",
      "Extracting topics [nodes: 67, num_workers: 4]: 100%|██████████| 67/67 [01:14<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 22:11:22:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [350], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:11:22:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [258], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:11:22:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [380], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:11:22:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [361], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:11:22:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [321], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-05-11 22:11:22:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [161], batch_writes_enabled: True, batch_write_size: 25]\n",
      "Stored collection batch-1747014631 in DynamoDB with status COMPLETED\n",
      "Extraction complete\n",
      "collection_id: batch-1747014631\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
