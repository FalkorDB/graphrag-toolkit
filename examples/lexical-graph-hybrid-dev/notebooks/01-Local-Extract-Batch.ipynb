{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "434fea4e",
   "metadata": {},
   "source": [
    "# 02 - Separate Extract and Build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb5cff",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven't already, install the toolkit and dependencies using the [Setup](./00-Setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfff49b",
   "metadata": {},
   "source": [
    "## Local extract to folder\n",
    "\n",
    "See [Run the extract and build stages separately](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/indexing.md#run-the-extract-and-build-stages-separately)"
   ]
  },
  {
   "cell_type": "code",
   "id": "79b6c7bd",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import FileBasedDocs\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "# Register the FalkorDB backend with the factory\n",
    "GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "extracted_docs = FileBasedDocs(\n",
    "    docs_directory='extracted'\n",
    ")\n",
    "\n",
    "checkpoint = Checkpoint('extraction-checkpoint')\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store, \n",
    "    vector_store\n",
    ")\n",
    "\n",
    "doc_urls = [\n",
    "    'https://docs.aws.amazon.com/neptune/latest/userguide/intro.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-features.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-vs-neptune-database.html'\n",
    "]\n",
    "\n",
    "docs = SimpleWebPageReader(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url:{'url': url}\n",
    ").load_data(doc_urls)\n",
    "\n",
    "graph_index.extract(docs, handler=extracted_docs, checkpoint=checkpoint, show_progress=True)\n",
    "\n",
    "collection_id = extracted_docs.collection_id\n",
    "\n",
    "print('Extraction complete')\n",
    "print(f'collection_id: {collection_id}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extraction to S3\n",
    "\n",
    "See [Run the extract and build stages separately](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/indexing.md#run-the-extract-and-build-stages-separately)"
   ],
   "id": "b5ed8c34f6f937fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import S3BasedDocs\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "# Register the FalkorDB backend with the factory\n",
    "GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "extracted_docs = S3BasedDocs(\n",
    "    region=os.environ['AWS_REGION'],\n",
    "    bucket_name=os.environ['S3_BUCKET_EXTRACK_BUILD_BATCH_NAME'],\n",
    "    key_prefix=os.environ[\"EXTRACT_BUILD_PREFIX\"],\n",
    "    collection_id='web-docs'\n",
    ")\n",
    "\n",
    "checkpoint = Checkpoint('s3-extraction-web-docs-checkpoint-01')\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store,\n",
    "    vector_store\n",
    ")\n",
    "\n",
    "doc_urls = [\n",
    "    'https://docs.aws.amazon.com/neptune/latest/userguide/intro.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-features.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-vs-neptune-database.html'\n",
    "]\n",
    "\n",
    "docs = SimpleWebPageReader(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url:{'url': url}\n",
    ").load_data(doc_urls)\n",
    "\n",
    "graph_index.extract(docs, handler=extracted_docs, checkpoint=checkpoint, show_progress=True)\n",
    "\n",
    "collection_id = extracted_docs.collection_id\n",
    "\n",
    "print('Extraction complete')\n",
    "print(f'collection_id: {collection_id}')"
   ],
   "id": "917e0345c606995f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using batch inference with the LexicalGraphIndex. Writing to AWS S3 and DynamoDB",
   "id": "fc6568641b03d2da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ensure you have reviewed batch-extraction.md. For permission creation please see setup-bedrock-batch.md in lexical-graph-hybrid-dev/aws folder.",
   "id": "6e19fc7db1fd8061"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T21:34:21.335162Z",
     "start_time": "2025-05-14T20:32:01.665814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import (\n",
    "    GraphRAGConfig,\n",
    "    IndexingConfig\n",
    "    )\n",
    "\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import S3BasedDocs\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "from graphrag_toolkit.lexical_graph.indexing.extract import BatchConfig\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "# Set batch size\n",
    "GraphRAGConfig.extraction_batch_size = int(os.environ.get(\"EXTRACTION_BATCH_SIZE\", 4))\n",
    "\n",
    "# Configure batch S3 setup\n",
    "batch_config = BatchConfig(\n",
    "        region=os.environ[\"AWS_REGION\"],\n",
    "        bucket_name=os.environ[\"S3_BUCKET_EXTRACK_BUILD_BATCH_NAME\"],\n",
    "        key_prefix=os.environ[\"BATCH_PREFIX\"],\n",
    "        role_arn=f'arn:aws:iam::{os.environ[\"AWS_ACCOUNT\"]}:role/{os.environ[\"BATCH_ROLE_NAME\"]}',\n",
    "    )\n",
    "\n",
    "indexing_config = IndexingConfig(batch_config=batch_config)\n",
    "\n",
    "# Register the FalkorDB backend with the factory\n",
    "GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "extracted_docs = S3BasedDocs(\n",
    "    region=os.environ['AWS_REGION'],\n",
    "    bucket_name=os.environ['S3_BUCKET_EXTRACK_BUILD_BATCH_NAME'],\n",
    "    key_prefix=os.environ[\"EXTRACT_BUILD_PREFIX\"],\n",
    "    collection_id='best-practices'\n",
    ")\n",
    "\n",
    "# Create checkpoint\n",
    "checkpoint = Checkpoint('extraction-best-practices-checkpoint-01')\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store,\n",
    "    vector_store,\n",
    "    indexing_config=indexing_config\n",
    ")\n",
    "\n",
    "# Use PyMuPDF for PDFs\n",
    "file_extractor = {\n",
    "        \".pdf\": PyMuPDFReader()\n",
    "}\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "        input_dir=os.environ[\"SOURCE_DIR\"],\n",
    "        file_extractor=file_extractor\n",
    ")\n",
    "\n",
    "docs = reader.load_data()\n",
    "\n",
    "graph_index.extract(docs, handler=extracted_docs, checkpoint=checkpoint, show_progress=True)\n",
    "\n",
    "collection_id = extracted_docs.collection_id\n",
    "\n",
    "print('Extraction complete')\n",
    "print(f'collection_id: {collection_id}')"
   ],
   "id": "13f0f063fc01eb06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 16:32:04:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 100, num_workers: 1]\n",
      "2025-05-14 16:32:06:INFO:g.l.i.u.batch_inference_utils:Created batch job [job_arn: arn:aws:bedrock:us-east-1:188967239867:model-invocation-job/81cq0745h02f]\n",
      "2025-05-14 17:06:13:INFO:g.l.i.u.batch_inference_utils:Created batch job [job_arn: arn:aws:bedrock:us-east-1:188967239867:model-invocation-job/l8wwvrz3ikiz]\n",
      "2025-05-14 17:34:21:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [668], batch_writes_enabled: True, batch_write_size: 25]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/save_points/extraction-best-practices-checkpoint-01/aws::e8de11c7:fae1:254d32fc'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31m_RemoteTraceback\u001B[39m                          Traceback (most recent call last)",
      "\u001B[31m_RemoteTraceback\u001B[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit6/lib/python3.12/concurrent/futures/process.py\", line 264, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit6/lib/python3.12/concurrent/futures/process.py\", line 213, in _process_chunk\n    return [fn(*args) for args in chunk]\n            ^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit6/lib/python3.12/site-packages/llama_index/core/ingestion/pipeline.py\", line 101, in run_transformations\n    nodes = transform(nodes, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit6/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 324, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit6/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/node_handler.py\", line 15, in __call__\n    return [n for n in self.accept(nodes, **kwargs)]\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit6/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/build/checkpoint.py\", line 60, in accept\n    self.touch(node_checkpoint_path)\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit6/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/build/checkpoint.py\", line 48, in touch\n    with open(path, 'a'):\n         ^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'output/save_points/extraction-best-practices-checkpoint-01/aws::e8de11c7:fae1:254d32fc'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 70\u001B[39m\n\u001B[32m     63\u001B[39m reader = SimpleDirectoryReader(\n\u001B[32m     64\u001B[39m         input_dir=os.environ[\u001B[33m\"\u001B[39m\u001B[33mSOURCE_DIR\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     65\u001B[39m         file_extractor=file_extractor\n\u001B[32m     66\u001B[39m )\n\u001B[32m     68\u001B[39m docs = reader.load_data()\n\u001B[32m---> \u001B[39m\u001B[32m70\u001B[39m graph_index.extract(docs, handler=extracted_docs, checkpoint=checkpoint, show_progress=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     72\u001B[39m collection_id = extracted_docs.collection_id\n\u001B[32m     74\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mExtraction complete\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit6/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/lexical_graph_index.py:292\u001B[39m, in \u001B[36mLexicalGraphIndex.extract\u001B[39m\u001B[34m(self, nodes, handler, checkpoint, show_progress, **kwargs)\u001B[39m\n\u001B[32m    280\u001B[39m build_pipeline = BuildPipeline.create(\n\u001B[32m    281\u001B[39m     components=[\n\u001B[32m    282\u001B[39m         NullBuilder()\n\u001B[32m   (...)\u001B[39m\u001B[32m    288\u001B[39m     **kwargs\n\u001B[32m    289\u001B[39m )\n\u001B[32m    291\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m handler:\n\u001B[32m--> \u001B[39m\u001B[32m292\u001B[39m     nodes | extraction_pipeline | Pipe(handler.accept) | build_pipeline | sink\n\u001B[32m    293\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    294\u001B[39m     nodes | extraction_pipeline | build_pipeline | sink\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit6/lib/python3.12/site-packages/pipe.py:42\u001B[39m, in \u001B[36mPipe.__ror__\u001B[39m\u001B[34m(self, other)\u001B[39m\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__ror__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.function(other)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit6/lib/python3.12/site-packages/pipe.py:36\u001B[39m, in \u001B[36mPipe.__init__.<locals>.<lambda>\u001B[39m\u001B[34m(iterable, *args2, **kwargs2)\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, function, *args, **kwargs):\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m     \u001B[38;5;28mself\u001B[39m.function = \u001B[38;5;28;01mlambda\u001B[39;00m iterable, *args2, **kwargs2: function(\n\u001B[32m     37\u001B[39m         iterable, *args, *args2, **kwargs, **kwargs2\n\u001B[32m     38\u001B[39m     )\n\u001B[32m     39\u001B[39m     functools.update_wrapper(\u001B[38;5;28mself\u001B[39m, function)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit6/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/utils/pipeline_utils.py:16\u001B[39m, in \u001B[36m_sink.<locals>._sink_from\u001B[39m\u001B[34m(generator)\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_sink_from\u001B[39m(generator):\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m generator:\n\u001B[32m     17\u001B[39m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit6/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/build/build_pipeline.py:164\u001B[39m, in \u001B[36mBuildPipeline.build\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m    160\u001B[39m node_batches:List[List[BaseNode]] = \u001B[38;5;28mself\u001B[39m._to_node_batches(source_doc_batches)\n\u001B[32m    162\u001B[39m logger.info(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mRunning build pipeline [batch_size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.batch_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, num_workers: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.num_workers\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, job_sizes: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m[\u001B[38;5;28mlen\u001B[39m(b)\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mb\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mnode_batches]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, batch_writes_enabled: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.batch_writes_enabled\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, batch_write_size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.batch_write_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m]\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m164\u001B[39m output_nodes = run_pipeline(\n\u001B[32m    165\u001B[39m     \u001B[38;5;28mself\u001B[39m.inner_pipeline,\n\u001B[32m    166\u001B[39m     node_batches,\n\u001B[32m    167\u001B[39m     num_workers=\u001B[38;5;28mself\u001B[39m.num_workers,\n\u001B[32m    168\u001B[39m     batch_writes_enabled=\u001B[38;5;28mself\u001B[39m.batch_writes_enabled,\n\u001B[32m    169\u001B[39m     batch_size=\u001B[38;5;28mself\u001B[39m.batch_size,\n\u001B[32m    170\u001B[39m     batch_write_size=\u001B[38;5;28mself\u001B[39m.batch_write_size,\n\u001B[32m    171\u001B[39m     include_domain_labels=\u001B[38;5;28mself\u001B[39m.include_domain_labels,\n\u001B[32m    172\u001B[39m     **\u001B[38;5;28mself\u001B[39m.pipeline_kwargs\n\u001B[32m    173\u001B[39m )\n\u001B[32m    175\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m output_nodes:\n\u001B[32m    176\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m node\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit6/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/utils/pipeline_utils.py:41\u001B[39m, in \u001B[36mrun_pipeline\u001B[39m\u001B[34m(pipeline, node_batches, cache_collection, in_place, num_workers, **kwargs)\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m ProcessPoolExecutor(max_workers=num_workers) \u001B[38;5;28;01mas\u001B[39;00m p:\n\u001B[32m     40\u001B[39m     processed_node_batches = p.map(transform, node_batches)\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     processed_nodes = \u001B[38;5;28msum\u001B[39m(processed_node_batches, start=cast(List[BaseNode], []))\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m processed_nodes\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit6/lib/python3.12/concurrent/futures/process.py:636\u001B[39m, in \u001B[36m_chain_from_iterable_of_lists\u001B[39m\u001B[34m(iterable)\u001B[39m\n\u001B[32m    630\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_chain_from_iterable_of_lists\u001B[39m(iterable):\n\u001B[32m    631\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    632\u001B[39m \u001B[33;03m    Specialized implementation of itertools.chain.from_iterable.\u001B[39;00m\n\u001B[32m    633\u001B[39m \u001B[33;03m    Each item in *iterable* should be a list.  This function is\u001B[39;00m\n\u001B[32m    634\u001B[39m \u001B[33;03m    careful not to keep references to yielded objects.\u001B[39;00m\n\u001B[32m    635\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m636\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m element \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[32m    637\u001B[39m         element.reverse()\n\u001B[32m    638\u001B[39m         \u001B[38;5;28;01mwhile\u001B[39;00m element:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit6/lib/python3.12/concurrent/futures/_base.py:619\u001B[39m, in \u001B[36mExecutor.map.<locals>.result_iterator\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m fs:\n\u001B[32m    617\u001B[39m     \u001B[38;5;66;03m# Careful not to keep a reference to the popped future\u001B[39;00m\n\u001B[32m    618\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m619\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m _result_or_cancel(fs.pop())\n\u001B[32m    620\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    621\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit6/lib/python3.12/concurrent/futures/_base.py:317\u001B[39m, in \u001B[36m_result_or_cancel\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    316\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m317\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m fut.result(timeout)\n\u001B[32m    318\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    319\u001B[39m         fut.cancel()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit6/lib/python3.12/concurrent/futures/_base.py:456\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    454\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[32m    455\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state == FINISHED:\n\u001B[32m--> \u001B[39m\u001B[32m456\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.__get_result()\n\u001B[32m    457\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    458\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit6/lib/python3.12/concurrent/futures/_base.py:401\u001B[39m, in \u001B[36mFuture.__get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    399\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception:\n\u001B[32m    400\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    403\u001B[39m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[32m    404\u001B[39m         \u001B[38;5;28mself\u001B[39m = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'output/save_points/extraction-best-practices-checkpoint-01/aws::e8de11c7:fae1:254d32fc'"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
