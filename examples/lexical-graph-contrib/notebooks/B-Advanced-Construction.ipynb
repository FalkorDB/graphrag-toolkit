{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c2a2f9",
   "metadata": {},
   "source": "# A - Advanced Construction"
  },
  {
   "cell_type": "markdown",
   "id": "4796b938",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven't already, install the toolkit and dependencies using the [Setup](./00-Setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e33c3e",
   "metadata": {},
   "source": [
    "## Extract and build pipelines\n",
    "\n",
    "See [Advanced graph construction](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/indexing.md#advanced-graph-construction)."
   ]
  },
  {
   "cell_type": "code",
   "id": "9c802bb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T15:09:58.023573Z",
     "start_time": "2025-05-06T15:08:40.071688Z"
    }
   },
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing import sink\n",
    "from graphrag_toolkit.lexical_graph.indexing.constants import PROPOSITIONS_KEY, DEFAULT_ENTITY_CLASSIFICATIONS\n",
    "from graphrag_toolkit.lexical_graph.indexing.extract import LLMPropositionExtractor\n",
    "from graphrag_toolkit.lexical_graph.indexing.extract import TopicExtractor\n",
    "from graphrag_toolkit.lexical_graph.indexing.extract import GraphScopedValueStore\n",
    "from graphrag_toolkit.lexical_graph.indexing.extract import ScopedValueProvider, DEFAULT_SCOPE\n",
    "from graphrag_toolkit.lexical_graph.indexing.extract import ExtractionPipeline\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import BuildPipeline\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import VectorIndexing\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import GraphConstruction\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = Checkpoint('advanced-construction-example', enabled=True)\n",
    "set_logging_config('INFO')\n",
    "\n",
    "\n",
    "# Register the FalkorDB backend with the factory\n",
    "GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "# Create graph and vector stores\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store,\n",
    "    vector_store\n",
    ")\n",
    "\n",
    "# Create extraction pipeline components\n",
    "\n",
    "# 1. Chunking using SentenceSplitter\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=350,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# 2. Proposition extraction\n",
    "proposition_extractor = LLMPropositionExtractor()\n",
    "\n",
    "# 3. Topic extraction\n",
    "entity_classification_provider = ScopedValueProvider(\n",
    "    label='EntityClassification',\n",
    "    scoped_value_store=GraphScopedValueStore(graph_store=graph_store),\n",
    "    initial_scoped_values = { DEFAULT_SCOPE: DEFAULT_ENTITY_CLASSIFICATIONS }\n",
    ")\n",
    "\n",
    "topic_extractor = TopicExtractor(\n",
    "    source_metadata_field=PROPOSITIONS_KEY, # Omit this line if not performing proposition extraction\n",
    "    entity_classification_provider=entity_classification_provider # Entity classifications saved to graph between LLM invocations\n",
    ")\n",
    "\n",
    "# Create extraction pipeline\n",
    "extraction_pipeline = ExtractionPipeline.create(\n",
    "    components=[\n",
    "        splitter, \n",
    "        proposition_extractor,\n",
    "        topic_extractor\n",
    "    ],\n",
    "    num_workers=2,\n",
    "    batch_size=4,\n",
    "    checkpoint=checkpoint,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Create build pipeline components\n",
    "graph_construction = GraphConstruction.for_graph_store(graph_store)\n",
    "vector_indexing = VectorIndexing.for_vector_store(vector_store)\n",
    "        \n",
    "# Create build pipeline        \n",
    "build_pipeline = BuildPipeline.create(\n",
    "    components=[\n",
    "        graph_construction,\n",
    "        vector_indexing\n",
    "    ],\n",
    "    num_workers=2,\n",
    "    batch_size=10,\n",
    "    batch_writes_enabled=True,\n",
    "    checkpoint=checkpoint,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Load source documents\n",
    "doc_urls = [\n",
    "    'https://docs.aws.amazon.com/neptune/latest/userguide/intro.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-features.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-vs-neptune-database.html'\n",
    "]\n",
    "\n",
    "docs = SimpleWebPageReader(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url:{'url': url}\n",
    ").load_data(doc_urls)\n",
    "\n",
    "# Run the build and exraction stages\n",
    "docs | extraction_pipeline | build_pipeline | sink\n",
    "\n",
    "print('Complete')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-06 11:08:42:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting propositions [nodes: 4, num_workers: 4]: 100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "Extracting propositions [nodes: 7, num_workers: 4]: 100%|██████████| 7/7 [00:19<00:00,  2.73s/it]\n",
      "Extracting topics [nodes: 4, num_workers: 4]: 100%|██████████| 4/4 [00:19<00:00,  4.85s/it]\n",
      "Extracting topics [nodes: 7, num_workers: 4]: 100%|██████████| 7/7 [00:54<00:00,  7.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-06 11:09:56:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 10, num_workers: 2, job_sizes: [425, 170], batch_writes_enabled: True, batch_write_size: 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 170/170 [00:00<00:00, 53797.47it/s]\n",
      "Building graph [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 425/425 [00:00<00:00, 51060.67it/s]\n",
      "Building vector index [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 170/170 [00:00<00:00, 915316.66it/s]\n",
      "Building vector index [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 425/425 [00:00<00:00, 1111333.67it/s]\n"
     ]
    },
    {
     "ename": "UndefinedObject",
     "evalue": "data type text has no default operator class for access method \"gin\"\nHINT:  You must specify an operator class for the index or define a default operator class for the data type.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31m_RemoteTraceback\u001B[39m                          Traceback (most recent call last)",
      "\u001B[31m_RemoteTraceback\u001B[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit5/lib/python3.12/concurrent/futures/process.py\", line 264, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit5/lib/python3.12/concurrent/futures/process.py\", line 213, in _process_chunk\n    return [fn(*args) for args in chunk]\n            ^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/llama_index/core/ingestion/pipeline.py\", line 100, in run_transformations\n    nodes = transform(nodes, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 322, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/node_handler.py\", line 15, in __call__\n    return [n for n in self.accept(nodes, **kwargs)]\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/build/checkpoint.py\", line 53, in accept\n    for node in self.inner.accept(nodes, **kwargs):\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/build/vector_indexing.py\", line 56, in accept\n    batch_nodes = batch_client.apply_batch_operations()\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/build/vector_batch_client.py\", line 56, in apply_batch_operations\n    index.write_embeddings_to_index()\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/build/vector_batch_client.py\", line 26, in write_embeddings_to_index\n    self.index.add_embeddings(nodes)\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/storage/vector/pg_vector_indexes.py\", line 264, in add_embeddings\n    dbconn = self._get_connection()\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/evanerwee/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/storage/vector/pg_vector_indexes.py\", line 246, in _get_connection\n    cur.execute(f'CREATE INDEX IF NOT EXISTS {index_name} ON {self.schema_name}.{self.underlying_index_name()} USING GIN (metadata)')\npsycopg2.errors.UndefinedObject: data type text has no default operator class for access method \"gin\"\nHINT:  You must specify an operator class for the index or define a default operator class for the data type.\n\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mUndefinedObject\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 111\u001B[39m\n\u001B[32m    105\u001B[39m docs = SimpleWebPageReader(\n\u001B[32m    106\u001B[39m     html_to_text=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    107\u001B[39m     metadata_fn=\u001B[38;5;28;01mlambda\u001B[39;00m url:{\u001B[33m'\u001B[39m\u001B[33murl\u001B[39m\u001B[33m'\u001B[39m: url}\n\u001B[32m    108\u001B[39m ).load_data(doc_urls)\n\u001B[32m    110\u001B[39m \u001B[38;5;66;03m# Run the build and exraction stages\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m111\u001B[39m docs | extraction_pipeline | build_pipeline | sink\n\u001B[32m    113\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mComplete\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/pipe.py:42\u001B[39m, in \u001B[36mPipe.__ror__\u001B[39m\u001B[34m(self, other)\u001B[39m\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__ror__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.function(other)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/pipe.py:36\u001B[39m, in \u001B[36mPipe.__init__.<locals>.<lambda>\u001B[39m\u001B[34m(iterable, *args2, **kwargs2)\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, function, *args, **kwargs):\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m     \u001B[38;5;28mself\u001B[39m.function = \u001B[38;5;28;01mlambda\u001B[39;00m iterable, *args2, **kwargs2: function(\n\u001B[32m     37\u001B[39m         iterable, *args, *args2, **kwargs, **kwargs2\n\u001B[32m     38\u001B[39m     )\n\u001B[32m     39\u001B[39m     functools.update_wrapper(\u001B[38;5;28mself\u001B[39m, function)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/utils/pipeline_utils.py:16\u001B[39m, in \u001B[36m_sink.<locals>._sink_from\u001B[39m\u001B[34m(generator)\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_sink_from\u001B[39m(generator):\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m generator:\n\u001B[32m     17\u001B[39m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/build/build_pipeline.py:154\u001B[39m, in \u001B[36mBuildPipeline.build\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m    150\u001B[39m node_batches:List[List[BaseNode]] = \u001B[38;5;28mself\u001B[39m._to_node_batches(source_doc_batches)\n\u001B[32m    152\u001B[39m logger.info(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mRunning build pipeline [batch_size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.batch_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, num_workers: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.num_workers\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, job_sizes: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m[\u001B[38;5;28mlen\u001B[39m(b)\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mb\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mnode_batches]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, batch_writes_enabled: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.batch_writes_enabled\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, batch_write_size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.batch_write_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m]\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m output_nodes = run_pipeline(\n\u001B[32m    155\u001B[39m     \u001B[38;5;28mself\u001B[39m.inner_pipeline,\n\u001B[32m    156\u001B[39m     node_batches,\n\u001B[32m    157\u001B[39m     num_workers=\u001B[38;5;28mself\u001B[39m.num_workers,\n\u001B[32m    158\u001B[39m     batch_writes_enabled=\u001B[38;5;28mself\u001B[39m.batch_writes_enabled,\n\u001B[32m    159\u001B[39m     batch_size=\u001B[38;5;28mself\u001B[39m.batch_size,\n\u001B[32m    160\u001B[39m     batch_write_size=\u001B[38;5;28mself\u001B[39m.batch_write_size,\n\u001B[32m    161\u001B[39m     include_domain_labels=\u001B[38;5;28mself\u001B[39m.include_domain_labels,\n\u001B[32m    162\u001B[39m     **\u001B[38;5;28mself\u001B[39m.pipeline_kwargs\n\u001B[32m    163\u001B[39m )\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m output_nodes:\n\u001B[32m    166\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m node\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit5/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/indexing/utils/pipeline_utils.py:41\u001B[39m, in \u001B[36mrun_pipeline\u001B[39m\u001B[34m(pipeline, node_batches, cache_collection, in_place, num_workers, **kwargs)\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m ProcessPoolExecutor(max_workers=num_workers) \u001B[38;5;28;01mas\u001B[39;00m p:\n\u001B[32m     40\u001B[39m     processed_node_batches = p.map(transform, node_batches)\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     processed_nodes = \u001B[38;5;28msum\u001B[39m(processed_node_batches, start=cast(List[BaseNode], []))\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m processed_nodes\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit5/lib/python3.12/concurrent/futures/process.py:636\u001B[39m, in \u001B[36m_chain_from_iterable_of_lists\u001B[39m\u001B[34m(iterable)\u001B[39m\n\u001B[32m    630\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_chain_from_iterable_of_lists\u001B[39m(iterable):\n\u001B[32m    631\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    632\u001B[39m \u001B[33;03m    Specialized implementation of itertools.chain.from_iterable.\u001B[39;00m\n\u001B[32m    633\u001B[39m \u001B[33;03m    Each item in *iterable* should be a list.  This function is\u001B[39;00m\n\u001B[32m    634\u001B[39m \u001B[33;03m    careful not to keep references to yielded objects.\u001B[39;00m\n\u001B[32m    635\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m636\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m element \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[32m    637\u001B[39m         element.reverse()\n\u001B[32m    638\u001B[39m         \u001B[38;5;28;01mwhile\u001B[39;00m element:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit5/lib/python3.12/concurrent/futures/_base.py:619\u001B[39m, in \u001B[36mExecutor.map.<locals>.result_iterator\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m fs:\n\u001B[32m    617\u001B[39m     \u001B[38;5;66;03m# Careful not to keep a reference to the popped future\u001B[39;00m\n\u001B[32m    618\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m619\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m _result_or_cancel(fs.pop())\n\u001B[32m    620\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    621\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit5/lib/python3.12/concurrent/futures/_base.py:317\u001B[39m, in \u001B[36m_result_or_cancel\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    316\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m317\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m fut.result(timeout)\n\u001B[32m    318\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    319\u001B[39m         fut.cancel()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit5/lib/python3.12/concurrent/futures/_base.py:456\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    454\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[32m    455\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state == FINISHED:\n\u001B[32m--> \u001B[39m\u001B[32m456\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.__get_result()\n\u001B[32m    457\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    458\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit5/lib/python3.12/concurrent/futures/_base.py:401\u001B[39m, in \u001B[36mFuture.__get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    399\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception:\n\u001B[32m    400\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    403\u001B[39m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[32m    404\u001B[39m         \u001B[38;5;28mself\u001B[39m = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[31mUndefinedObject\u001B[39m: data type text has no default operator class for access method \"gin\"\nHINT:  You must specify an operator class for the index or define a default operator class for the data type.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "5288cdd2",
   "metadata": {},
   "source": [
    "from graphrag_toolkit.lexical_graph import GraphRAGConfig\n",
    "print(f\"GraphRAGConfig: {GraphRAGConfig}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
