{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40dfbd7",
   "metadata": {},
   "source": [
    "# Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d9b60c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven't already, install the toolkit and dependencies using the [Setup](./00-Setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d08c74",
   "metadata": {},
   "source": [
    "### TraversalBasedRetriever\n",
    "\n",
    "See [TraversalBasedRetriever](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/querying.md#traversalbasedretriever)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check GPU support on Ubuntu 24.04 LTS",
   "id": "9bec686fd2672567"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from sympy.codegen.fnodes import dimension\n",
    "\n",
    "# Force inject paths (adapt to your system if different)\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "os.environ[\"PATH\"] += \":/usr/local/cuda/bin\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"EMBEDDINGS_MODEL\"] = \"cohere.embed-english-v3\"\n",
    "os.environ[\"EMBEDDINGS_DIMENSIONS\"] = \"1024\"\n",
    "\n",
    "# Re-check CUDA\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Version (Torch):\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"GPU still not detected inside notebook\")"
   ],
   "id": "2b7107a66a3d6607",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup AWS Profile",
   "id": "cf73d0b003bf1179"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T02:21:35.743039Z",
     "start_time": "2025-05-06T02:21:35.740921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure AWS Profile and Region\n",
    "from graphrag_toolkit.lexical_graph import GraphRAGConfig\n",
    "\n",
    "# Assign profile and region to GraphRAGConfig\n",
    "GraphRAGConfig.aws_profile = \"padmin\" #Optional, use if using AWS SSO\n",
    "GraphRAGConfig.aws_region = \"us-east-1\""
   ],
   "id": "d0895acf6fbc98f1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup model",
   "id": "d817c4c9c68f116b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Claude model via Bedrock\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\""
   ],
   "id": "4246fd3faec4a7e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup BedrockConverse",
   "id": "1f3e98634dc4c67d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configure BedrockConverse\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "\n",
    "try:\n",
    "    GraphRAGConfig.extraction_llm = BedrockConverse.from_json(f'''\n",
    "    {{\n",
    "        \"model\": \"{model_id}\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"profile_name\": \"{GraphRAGConfig.aws_profile}\",\n",
    "        \"region_name\": \"{GraphRAGConfig.aws_region}\"\n",
    "    }}\n",
    "    ''')\n",
    "    print(f\"Successfully configured Bedrock model: {model_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize BedrockConverse: {str(e)}\")\n",
    "    raise\n",
    "### Display LLM Configuration"
   ],
   "id": "3e1dcab3be3bf81a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Display LLM Configuration",
   "id": "23ad3e6f57be3dee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display LLM configuration\n",
    "llm = GraphRAGConfig.extraction_llm\n",
    "print(\"LLM class:\", llm.__class__.__name__)\n",
    "print(\"Model ID:\", llm.model)\n",
    "print(\"Temperature:\", llm.temperature)\n",
    "print(\"Max tokens:\", llm.max_tokens)\n",
    "print(\"Profile:\", getattr(llm, 'profile_name', None))\n",
    "print(\"Region:\", getattr(llm, 'region_name', None))"
   ],
   "id": "cd6de0bcdbaf30c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"embed_model model name = {GraphRAGConfig.embed_model.model_name}\")\n",
    "print(f\"embed_model dimension  = {GraphRAGConfig.embed_dimensions}\")"
   ],
   "id": "d71998f8626eb2e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup connection with PostgreSQL",
   "id": "b03ac415378e2820"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T02:21:39.344790Z",
     "start_time": "2025-05-06T02:21:39.298082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to PostgreSQL Vector Store\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "\n",
    "# PostgreSQL connection string\n",
    "postgre_connection_info = 'postgresql://graphrag:graphragpass@localhost:5432/graphrag_db'\n",
    "\n",
    "# Instantiate vector store using factory\n",
    "vector_store = VectorStoreFactory.for_vector_store(postgre_connection_info)\n",
    "\n",
    "# Optional: confirm\n",
    "print(f\"Vector store initialized: {vector_store}\")"
   ],
   "id": "862a9fe3c0c61e6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized: indexes={'chunk': PGIndex(index_name='chunk', tenant_id=TenantId(value=None), writeable=True, database='graphrag_db', schema_name='graphrag', host='localhost', port=5432, username='graphrag', password='graphragpass', dimensions=1024, embed_model=BedrockEmbedding(model_name='cohere.embed-english-v3', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7111a55f8c80>, num_workers=None, profile_name='padmin', aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None, region_name='us-east-1', botocore_session=None, botocore_config=None, max_retries=10, timeout=60.0, additional_kwargs={}), enable_iam_db_auth=False, initialized=False), 'statement': PGIndex(index_name='statement', tenant_id=TenantId(value=None), writeable=True, database='graphrag_db', schema_name='graphrag', host='localhost', port=5432, username='graphrag', password='graphragpass', dimensions=1024, embed_model=BedrockEmbedding(model_name='cohere.embed-english-v3', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7111a55f8c80>, num_workers=None, profile_name='padmin', aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None, region_name='us-east-1', botocore_session=None, botocore_config=None, max_retries=10, timeout=60.0, additional_kwargs={}), enable_iam_db_auth=False, initialized=False)}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup connection with FalkorDB",
   "id": "5fdb3f1e78c21337"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.5.0.zip#subdirectory=lexical-graph-contrib/falkordb",
   "id": "4c3c51fc31e11696",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T02:21:41.589096Z",
     "start_time": "2025-05-06T02:21:41.552099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to FalkorDB Graph Store\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "\n",
    "# Connection string for FalkorDB\n",
    "falkordb_connection_info = 'falkordb://localhost:6379'\n",
    "\n",
    "# Register the FalkorDB backend with the factory\n",
    "GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "# Instantiate a graph store using the factory\n",
    "graph_store = GraphStoreFactory.for_graph_store(falkordb_connection_info)\n",
    "\n",
    "# Optional: confirm initialization\n",
    "print(f\"FalkorDB GraphStore initialized: {graph_store}\")"
   ],
   "id": "a91526d4dd20c426",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalkorDB GraphStore initialized: log_formatting=RedactedGraphQueryLogFormatting() tenant_id=TenantId(value=None) endpoint_url='localhost:6379' database='graphrag' username=None password=None ssl=False\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "2ec2a722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T02:22:02.256425Z",
     "start_time": "2025-05-06T02:21:58.328253Z"
    }
   },
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import set_logging_config\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.tenant_id import TenantId\n",
    "from graphrag_toolkit.lexical_graph.retrieval.post_processors import SentenceReranker, SentenceReranker, StatementDiversityPostProcessor, StatementEnhancementPostProcessor\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store,\n",
    "    vector_store,\n",
    "    tenant_id=TenantId(\"dev123\"),\n",
    "\n",
    "    #post_processors=[\n",
    "    #    SentenceReranker(),\n",
    "    #    StatementDiversityPostProcessor(),\n",
    "    #    StatementEnhancementPostProcessor()\n",
    "    #]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Index statement_dev123 does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I do not have enough information from the provided search results to accurately answer your question about the differences between Neptune Database and Neptune Analytics. The search results are empty, so I cannot make any factual statements or comparisons between these two services. To provide a proper response, I would need relevant and reliable information about both Neptune Database and Neptune Analytics.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T02:22:25.818429Z",
     "start_time": "2025-05-06T02:22:08.898684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import RerankingBeamGraphSearch, StatementCosineSimilaritySearch, KeywordRankingSearch\n",
    "from graphrag_toolkit.lexical_graph.retrieval.post_processors import SentenceReranker\n",
    "\n",
    "\n",
    "cosine_retriever = StatementCosineSimilaritySearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "keyword_retriever = KeywordRankingSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    max_keywords=10\n",
    ")\n",
    "\n",
    "reranker = SentenceReranker(\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "beam_retriever = RerankingBeamGraphSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    reranker=reranker,\n",
    "    initial_retrievers=[cosine_retriever, keyword_retriever],\n",
    "    max_depth=8,\n",
    "    beam_width=100\n",
    ")\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store,\n",
    "    vector_store,\n",
    "    retrievers=[\n",
    "        cosine_retriever,\n",
    "        keyword_retriever,\n",
    "        beam_retriever\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "id": "79993fcfb2d6663b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Index statement_dev123 does not exist\n",
      "Index statement_dev123 does not exist\n",
      "Index statement_dev123 does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neptune Database and Neptune Analytics are two distinct components of Amazon Neptune, each designed for different purposes and use cases. Here are the key differences between them:\n",
      "\n",
      "1. Purpose:\n",
      "Neptune Database is a serverless graph database designed for operational workloads [source_1]. It's suitable for applications like social networking, fraud alerting, and Customer 360 [source_1].\n",
      "\n",
      "Neptune Analytics, on the other hand, is an analytics database engine focused on getting insights and finding trends in graph data [source_1, source_2].\n",
      "\n",
      "2. Data Processing:\n",
      "Neptune Database is optimized for transactional processing and can scale to handle 100,000 queries per second [source_1].\n",
      "\n",
      "Neptune Analytics is designed to quickly analyze large amounts of graph data in memory, processing thousands of analytic queries per second [source_1, source_2].\n",
      "\n",
      "3. Algorithms and Queries:\n",
      "Neptune Analytics uses popular graph analytic algorithms and supports low-latency analytic queries [source_1, source_2]. It provides a library of optimized graph analytic algorithms and supports vector search capabilities within graph traversals [source_2].\n",
      "\n",
      "Neptune Database, being an operational database, focuses on standard graph queries rather than analytical algorithms.\n",
      "\n",
      "4. Data Storage:\n",
      "Neptune Database is a persistent database that stores data on disk with multi-Region deployments and Multi-AZ high availability [source_1].\n",
      "\n",
      "Neptune Analytics stores large graph datasets in memory for fast processing [source_2].\n",
      "\n",
      "5. Use Cases:\n",
      "Neptune Database is used for operational applications that require continuous read/write operations [source_1].\n",
      "\n",
      "Neptune Analytics is ideal for data science workloads, investigatory and exploratory tasks, and workloads requiring fast iteration for analytical, data, and algorithmic processing [source_2].\n",
      "\n",
      "6. Data Loading:\n",
      "Neptune Analytics can load data from various sources, including Neptune Database snapshots, live Neptune Database graphs, and graph data stored in Amazon S3 [source_2].\n",
      "\n",
      "7. Management:\n",
      "Neptune Database is a fully managed database service that handles setup, configuration, and maintenance tasks [source_3].\n",
      "\n",
      "Neptune Analytics is a managed environment for graph analytics that automatically provisions compute resources based on graph size [source_4].\n",
      "\n",
      "In summary, Neptune Database is for operational graph database needs, while Neptune Analytics complements it by providing powerful analytical capabilities for deriving insights from graph data [source_2].\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "3b5a947e",
   "metadata": {},
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb78418c",
   "metadata": {},
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41035822",
   "metadata": {},
   "source": [
    "#### Set subretriever\n",
    "\n",
    "In the example below, the `TraversalBasedRetriever` is configured with a `ChunkBasedSearch` subretriever."
   ]
  },
  {
   "cell_type": "code",
   "id": "90d04b97",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import ChunkBasedSearch\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_traversal_based_search(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    retrievers=[ChunkBasedSearch]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d174f696",
   "metadata": {},
   "source": [
    "### SemanticGuidedRetriever\n",
    "\n",
    "See [SemanticGuidedRetriever](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/querying.md#semanticguidedretriever)."
   ]
  },
  {
   "cell_type": "code",
   "id": "9d1f3184",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store, \n",
    "    vector_store\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3416845",
   "metadata": {},
   "source": [
    "#### Set subretrievers"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec1adddb",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import StatementCosineSimilaritySearch, KeywordRankingSearch, SemanticBeamGraphSearch\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    retrievers=[\n",
    "        StatementCosineSimilaritySearch, \n",
    "        KeywordRankingSearch, \n",
    "        SemanticBeamGraphSearch\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b234dc61",
   "metadata": {},
   "source": [
    "#### Reranking beam search (CPU)\n",
    "\n",
    "The example below uses a `SentenceReranker` with a `RerankingBeamGraphSearch` to rerank statements while conducting the beam search."
   ]
  },
  {
   "cell_type": "code",
   "id": "902acaef",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import RerankingBeamGraphSearch, StatementCosineSimilaritySearch, KeywordRankingSearch\n",
    "from graphrag_toolkit.lexical_graph.retrieval.post_processors import SentenceReranker\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "cosine_retriever = StatementCosineSimilaritySearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "keyword_retriever = KeywordRankingSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    max_keywords=10\n",
    ")\n",
    "\n",
    "reranker = SentenceReranker(\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "beam_retriever = RerankingBeamGraphSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    reranker=reranker,\n",
    "    initial_retrievers=[cosine_retriever, keyword_retriever],\n",
    "    max_depth=8,\n",
    "    beam_width=100\n",
    ")\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    retrievers=[\n",
    "        cosine_retriever,\n",
    "        keyword_retriever,\n",
    "        beam_retriever\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "717049be",
   "metadata": {},
   "source": [
    "#### Reranking beam search (GPU)\n",
    "\n",
    "The example below uses a `BGEReranker` with a `RerankingBeamGraphSearch` to rerank statements while conducting the beam search.\n",
    "\n",
    "There will be a delay the first time this runs while the reranker downloads tensors."
   ]
  },
  {
   "cell_type": "code",
   "id": "06c3b363",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import RerankingBeamGraphSearch, StatementCosineSimilaritySearch, KeywordRankingSearch\n",
    "from graphrag_toolkit.lexical_graph.retrieval.post_processors.bge_reranker import BGEReranker\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "cosine_retriever = StatementCosineSimilaritySearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "keyword_retriever = KeywordRankingSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    max_keywords=10\n",
    ")\n",
    "\n",
    "reranker = BGEReranker(\n",
    "    gpu_id=0,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "beam_retriever = RerankingBeamGraphSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    reranker=reranker,\n",
    "    initial_retrievers=[cosine_retriever, keyword_retriever],\n",
    "    max_depth=8,\n",
    "    beam_width=100\n",
    ")\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    retrievers=[\n",
    "        cosine_retriever,\n",
    "        keyword_retriever,\n",
    "        beam_retriever\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ba2ac35",
   "metadata": {},
   "source": [
    "#### Post-processors \n",
    "\n",
    "The example below uses a `StatementDiversityPostProcessor`, `SentenceReranker` and `StatementEnhancementPostProcessor`.\n",
    "\n",
    "  - `SentenceReranker` - Reranks statements using the `mixedbread-ai/mxbai-rerank-xsmall-v1` model. \n",
    "\n",
    "  - `StatementEnhancementPostProcessor` - Enhances statements by using chunk context and an LLM to improve content while preserving original metadata.\n",
    "\n",
    "  - `StatementDiversityPostProcessor` - Removes similar statements using TF-IDF similarity with a default threshold of 0.975 to ensure diversity in the processed nodes.\n",
    "\n",
    "Before running `StatementDiversityPostProcessor` for the first time, load the following package:\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "If you're running on a GPU device, you can replace the `SentenceReranker` with a `BGEReranker`, which reranks statements using the ``BAAI/bge-reranker-v2-minicpm-layerwise`` model."
   ]
  },
  {
   "cell_type": "code",
   "id": "1f2d46a3",
   "metadata": {},
   "source": [
    "!python -m spacy download en_core_web_sm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef08819d",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.retrieval.post_processors import SentenceReranker, StatementDiversityPostProcessor, StatementEnhancementPostProcessor\n",
    "import os\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    post_processors=[\n",
    "        SentenceReranker(), \n",
    "        StatementDiversityPostProcessor(), \n",
    "        StatementEnhancementPostProcessor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
